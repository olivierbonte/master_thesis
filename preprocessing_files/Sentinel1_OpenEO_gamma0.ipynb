{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenEO: Sentinel 1, $\\gamma_0$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection openeo cloud backend for cloud computing (try my early adopter programme). In this way, processing to $\\gamma_0$ RTC can be conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import hvplot.xarray\n",
    "import hvplot.dask\n",
    "import os\n",
    "import time\n",
    "#connection = openeo.connect(\"openeo.cloud\").authenticate_oidc()\n",
    "connection = openeo.connect(\"openeo.vito.be\").authenticate_oidc()\n",
    "#connection = openeo.connect(\"openeocloud.vito.be\").authenticate_oidc()\n",
    "#connection = openeo.connect(\"openeocloud-dev.vito.be\").authenticate_oidc()\n",
    "pad = Path(os.getcwd())\n",
    "if pad.name == \"preprocessing_files\":\n",
    "    pad_correct = pad.parent\n",
    "    os.chdir(pad_correct)\n",
    "#set all of the parameters below to True to execute cloud computing and downloading    \n",
    "overwrite = True\n",
    "read = True\n",
    "job_exec =  True\n",
    "download = True\n",
    "write_out = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the Shapefile of the Zwalm in EPSG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/Zwalm_shape/zwalm_shapefile_emma.shp'):\n",
    "    %run \"preprocessing_files/shapefile_conversion.py\"   \n",
    "    print('Preprocessing script of Zwalm shapefile has run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_zwalm = gpd.read_file('data/Zwalm_shape/zwalm_shapefile_emma.shp')\n",
    "shape_zwalm.plot()\n",
    "extent = shape_zwalm.total_bounds\n",
    "print(extent)\n",
    "#[ 3.66751526 50.76325563  3.83821038 50.90341411] (give this extent of shapefile not on disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.describe_collection('SENTINEL1_GRD')\n",
    "#connection.describe_collection('S1_GRD_SIGMA0_ASCENDING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the temporal extent in to 1 year at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_extent = [\"2015-06-07\", \"2022-11-05\"]\n",
    "list_temp_extent = []\n",
    "job_title_list = []\n",
    "job_title = \"s1_a_gamma0_2015\" \n",
    "job_title_list.append(job_title)\n",
    "list_temp_extent.append([temporal_extent[0],\"2015-12-31\"])\n",
    "years = np.arange(2016,2023)\n",
    "for year in np.arange(2016,2023):\n",
    "    if year == 2022:\n",
    "        #print([str(year)+\"-01-01\",temporal_extent[1]])\n",
    "        list_temp_extent.append([str(year)+\"-01-01\",str(year)+ \"-06-30\"])\n",
    "        job_title = \"s1_a_gamma0_2022_I\"\n",
    "        job_title_list.append(job_title)\n",
    "        list_temp_extent.append([str(year)+\"-07-01\",temporal_extent[1]])\n",
    "        job_title = \"s1_a_gamma0_2022_II\" \n",
    "        job_title_list.append(job_title)\n",
    "    else:\n",
    "        #print([str(year)+\"-01-01\",str(year)+ \"-12-31\"])\n",
    "        list_temp_extent.append([str(year)+\"-01-01\",str(year)+ \"-06-30\"])\n",
    "        job_title = \"s1_a_gamma0_\" +  str(year) + \"_I\"\n",
    "        job_title_list.append(job_title)\n",
    "        list_temp_extent.append([str(year)+\"-07-01\",str(year) + \"-12-31\"])\n",
    "        job_title = \"s1_a_gamma0_\" +  str(year) + \"_II\"\n",
    "        job_title_list.append(job_title)\n",
    "print(list_temp_extent)\n",
    "print(job_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_temp_extent))\n",
    "print(len(job_title_list))\n",
    "print(list_temp_extent[2:-1])\n",
    "#display(connection.list_jobs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ascending orbit(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for filtering: https://docs.sentinel-hub.com/api/latest/data/sentinel-1-grd/#filter-extension\n",
    "\n",
    " https://docs.openeo.cloud/data-collections/ check here for correct filtering e.g. sar:...  or sat:... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'SENTINEL1_GRD' #Ground Range Detected #Ground Range Detected\n",
    "spatial_extent = {'west':extent[0],'east':extent[2],'south':extent[1],'north':extent[3]}\n",
    "bands = [\"VV\",\"VH\"]#enkel in deze geÃ¯nteresseerd \n",
    "properties = {\n",
    "    \"sat:orbit_state\": lambda od: od == \"ASCENDING\", ##filter on ascending vs descending\n",
    "    \"sar:instrument_mode\":lambda mode: mode == \"IW\", ## Orbit direction filtering\n",
    "    \"polarization\": lambda p: p == \"DV\"\n",
    "    #\"sar:polarizations\": lambda p: p == \"DV\", ## Suggestion Jeroen 27/02/2023\n",
    "    #\"s1:polarization\": lambda p: p == \"DV\", ## Suggestion Jeroen 27/02/2023\n",
    "    #\"s1:resolution\": lambda res : res == \"HIGH\" ## 10 m resolution for IW\n",
    " }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.openeo.cloud/processes/#sar_backscatter \n",
    "\n",
    "Sentinel-1 GRD provided by Sentinel Hub: https://docs.sentinel-hub.com/api/latest/data/sentinel-1-grd/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_list = []\n",
    "if job_exec:\n",
    "    for i, temporal_extent in enumerate(list_temp_extent):\n",
    "        #if i >= 2: #temporary since first 2 already worked! \n",
    "        s1a = connection.load_collection(\n",
    "            collection_id = collection,\n",
    "            spatial_extent= spatial_extent,\n",
    "            temporal_extent = temporal_extent,\n",
    "            bands = bands,\n",
    "            properties= properties\n",
    "        )\n",
    "        #s1a = s1a.ard_normalized_radar_backscatter(elevation_model = \"COPERNICUS_30\")\n",
    "        s1a = s1a.sar_backscatter(\n",
    "            coefficient  = \"gamma0-terrain\", #default\n",
    "            local_incidence_angle  = True,\n",
    "            elevation_model = \"COPERNICUS_30\"\n",
    "        ) #suggestion Jeroen\n",
    "        s1a = s1a.mask_polygon(shape_zwalm['geometry'].values[0])\n",
    "        # job_title = \"s1_a_gamm0\" +  str(years[i])\n",
    "        # job_title_list.append(job_title)\n",
    "        job_s1a = s1a.create_job(title = job_title_list[i], out_format= 'NetCDF')\n",
    "        job_s1a_id = job_s1a.job_id\n",
    "        if job_s1a_id:\n",
    "            print(\"Batch job created with id: \",job_s1a_id)\n",
    "            #job_s1a.start_and_wait()\n",
    "            job_s1a.start_job()\n",
    "            job_id_list.append(job_s1a_id)\n",
    "            time.sleep(40) # to prevent overloading the SentinelHub server\n",
    "        else:\n",
    "            print(\"Error! Job ID is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/g0_OpenEO/ascending'):\n",
    "    os.makedirs('data/g0_OpenEO/ascending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite:\n",
    "    with open('data/g0_OpenEO/s1_a_job_id_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(job_id_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/g0_OpenEO/s1_a_job_title_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(job_title_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read:\n",
    "    job_id_list = pickle.load(open('data/g0_OpenEO/s1_a_job_id_list.pickle', \"rb\"))\n",
    "    job_title_list = pickle.load(open('data/g0_OpenEO/s1_a_job_title_list.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    for i,job_id in enumerate(job_id_list):\n",
    "        job_connection = connection.job(job_id)\n",
    "        results = job_connection.get_results()\n",
    "        name_netcdf = job_title_list[i] + '.nc'\n",
    "        filepath = \"data/g0_OpenEO/ascending/\" + name_netcdf\n",
    "        print(filepath)\n",
    "        while job_connection.status() != 'finished':\n",
    "            time.sleep(30)\n",
    "            if job_connection.status() == 'error':\n",
    "                raise ChildProcessError(job_id + 'has encountered an error, check why batch job failed')\n",
    "            if job_connection.status() == 'canceled':\n",
    "                        raise ChildProcessError(job_id + 'has been canceled')\n",
    "        results.download_file(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Read in the downloaded data! \n",
    "- scale and offset from https://docs.terrascope.be/DataProducts/Sentinel-1/references/VITO_S1_sigma0_GRD.pdf  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_asc = xr.open_mfdataset('data/g0_OpenEO/ascending/*.nc', decode_coords=\"all\") #automatically chuncked!\n",
    "s1_xr_asc_plot = s1_xr_asc.copy()\n",
    "s1_xr_asc_plot['VV_db'] = 10 * np.log10(s1_xr_asc['VV'])\n",
    "# scale = 0.0005\n",
    "# offset = 29\n",
    "# s1_xr_asc['angle']  = s1_xr_asc['angle']*scale + offset   \n",
    "s1_xr_asc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only select the values for which sufficient data is present. First check amount of data in a full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_asc_plot['VV_db'].isel(t=0).plot() #example of a full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_asc['local_incidence_angle'].isel(t=0).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_shape = s1_xr_asc['VV'].shape\n",
    "print('Shape of the ascending orbits: ' + str(xr_shape))\n",
    "nr_pixels = xr_shape[1]*xr_shape[2]\n",
    "print('Number of pixels per timestamp: ' + str(nr_pixels))\n",
    "nancount = np.sum(np.isnan(s1_xr_asc['VV'].isel(t=0))).values\n",
    "print('Number of nan-pixels for a full image: ' + str(nancount))\n",
    "nan_cutoff = nancount/nr_pixels\n",
    "print('Percentage of nan-pixels in a full image: ' + str(nan_cutoff*100) + '%')\n",
    "#add 5% as safety margin to cutoff\n",
    "nan_cutoff = nan_cutoff + 0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_shape = s1_xr_asc['VV'].shape\n",
    "print('Shape of the ascending orbits: ' + str(xr_shape))\n",
    "nr_pixels = xr_shape[1]*xr_shape[2]\n",
    "print('Number of pixels per timestamp: ' + str(nr_pixels))\n",
    "nancount = np.sum(np.isnan(s1_xr_asc['VV'].isel(t=0))).values\n",
    "print('Number of nan-pixels for a full image: ' + str(nancount))\n",
    "nan_cutoff = nancount/nr_pixels\n",
    "print('Percentage of nan-pixels in a full image: ' + str(nan_cutoff*100) + '%')\n",
    "#add 5% as safety margin to cutoff\n",
    "nan_cutoff = nan_cutoff + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VV\n",
    "nr_timestemps_ascending = xr_shape[0]\n",
    "bool_full_image = []\n",
    "for i in range(nr_timestemps_ascending):\n",
    "    VV_ds = s1_xr_asc['VV'].isel(t=i)\n",
    "    temp_nancount = np.sum(np.isnan(VV_ds)).values\n",
    "    nan_frac = temp_nancount/nr_pixels\n",
    "    if nan_frac > nan_cutoff:\n",
    "        bool_full_image.append(0)\n",
    "    else:\n",
    "        bool_full_image.append(1)\n",
    "\n",
    "#VH\n",
    "bool_full_image_VH = []\n",
    "for i in range(nr_timestemps_ascending):\n",
    "    VV_ds = s1_xr_asc['VH'].isel(t=i)\n",
    "    temp_nancount = np.sum(np.isnan(VV_ds)).values\n",
    "    nan_frac = temp_nancount/nr_pixels\n",
    "    if nan_frac > nan_cutoff:\n",
    "        bool_full_image_VH.append(0)\n",
    "    else:\n",
    "        bool_full_image_VH.append(1)\n",
    "\n",
    "#Compare\n",
    "if not bool_full_image_VH == bool_full_image:\n",
    "    pos_emp_VH = np.where(np.array(bool_full_image_VH) == 0)[0].tolist()\n",
    "    bool_full_image_np = np.array(bool_full_image)\n",
    "    bool_full_image_np[pos_emp_VH] = 0\n",
    "    bool_full_image_all = bool_full_image_np.tolist()\n",
    "    print('VH and VV Nans were not equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_full = np.where(bool_full_image)[0].tolist()\n",
    "print(pos_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add orbit direction\n",
    "da = xr.DataArray(\n",
    "    data = np.repeat('ascending',nr_timestemps_ascending),\n",
    "    dims = ['t'],\n",
    "    coords = dict(t = s1_xr_asc['t'].values)\n",
    ")\n",
    "s1_xr_asc['Orbitdirection'] = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_asc_full = s1_xr_asc.isel(t = pos_full)\n",
    "s1_xr_asc_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_asc_full_plot = s1_xr_asc_full.copy()\n",
    "s1_xr_asc_full_plot['VV_db'] = 10 * np.log10(s1_xr_asc_full['VV'])\n",
    "s1_xr_asc_full_plot['VV_db'].hvplot.image('x','y', geo = True, crs = 32631, tiles = 'OSM', cmap = 'bwr', width = 400, rasterize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending orbit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    \"sat:orbit_state\": lambda od: od == \"DESCENDING\", ##filter on ascending vs descending\n",
    "    \"sar:instrument_mode\":lambda mode: mode == \"IW\", ## Orbit direction filtering\n",
    "    \"polarization\": lambda p: p == \"DV\"\n",
    "    #\"sar:polarizations\": lambda p: p == \"DV\", ## Suggestion Jeroen 27/02/2023\n",
    "    #\"s1:polarization\": lambda p: p == \"DV\", ## Suggestion Jeroen 27/02/2023\n",
    "    #\"s1:resolution\": lambda res : res == \"HIGH\" ## 10 m resolution for IW\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2015,2023)\n",
    "years_name_list = []\n",
    "for i, year in enumerate(years):\n",
    "    if i > 0:\n",
    "        years_name_list.append(str(year)+ '_I')\n",
    "        years_name_list.append(str(year) + '_II')\n",
    "    else:\n",
    "        years_name_list.append(str(year))\n",
    "print(years_name_list)\n",
    "print(len(years_name_list) == len(list_temp_extent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_list_d = []\n",
    "job_id_list_d = []\n",
    "if job_exec:\n",
    "    for i, temporal_extent in enumerate(list_temp_extent):\n",
    "        s1d = connection.load_collection(\n",
    "            collection_id = collection,\n",
    "            spatial_extent= spatial_extent,\n",
    "            temporal_extent = temporal_extent,\n",
    "            bands = bands,\n",
    "            properties= properties\n",
    "        )\n",
    "        #s1a = s1a.ard_normalized_radar_backscatter(elevation_model = \"COPERNICUS_30\")\n",
    "        s1d = s1d.sar_backscatter(\n",
    "            coefficient  = \"gamma0-terrain\", #default\n",
    "            local_incidence_angle  = True,\n",
    "            elevation_model = \"COPERNICUS_30\"\n",
    "        ) #suggestion Jeroen\n",
    "        s1d = s1d.mask_polygon(shape_zwalm['geometry'].values[0])\n",
    "        job_title = \"s1_d_gamma0-\" +  years_name_list[i]\n",
    "        job_title_list_d.append(job_title)\n",
    "        job_s1d = s1d.create_job(title = job_title, out_format= 'NetCDF')\n",
    "        job_s1d_id = job_s1d.job_id\n",
    "        if job_s1d_id:\n",
    "            print(\"Batch job created with id: \",job_s1a_id)\n",
    "            #job_s1a.start_and_wait()\n",
    "            job_s1d.start_job()\n",
    "            job_id_list_d.append(job_s1d_id)\n",
    "            time.sleep(40) # to prevent overloading the SentinelHub server\n",
    "        else:\n",
    "            print(\"Error! Job ID is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMP\n",
    "# len(list_temp_extent)\n",
    "# len(job_id_list)\n",
    "# job_id_list_d = job_id_list[-15:]\n",
    "# job_id_list_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite:\n",
    "    with open('data/g0_OpenEO/s1_d_job_id_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(job_id_list_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/g0_OpenEO/s1_d_job_title_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(job_title_list_d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read:\n",
    "    job_id_list_d = pickle.load(open('data/g0_OpenEO/s1_d_job_id_list.pickle', \"rb\"))\n",
    "    job_title_list_d = pickle.load(open('data/g0_OpenEO/s1_d_job_title_list.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/g0_OpenEO/descending'):\n",
    "    os.makedirs('data/g0_OpenEO/descending')\n",
    "if download:\n",
    "    for i,job_id in enumerate(job_id_list_d):\n",
    "        job_connection = connection.job(job_id)\n",
    "        results = job_connection.get_results()\n",
    "        name_netcdf = job_title_list_d[i] + '.nc'\n",
    "        filepath = \"data/g0_OpenEO/descending/\" + name_netcdf\n",
    "        print(filepath)\n",
    "        while job_connection.status() != 'finished':\n",
    "            time.sleep(30)\n",
    "            if job_connection.status() == 'error':\n",
    "                raise ChildProcessError(job_id + 'has encountered an error, check why batch job failed')\n",
    "            if job_connection.status() == 'canceled':\n",
    "                        raise ChildProcessError(job_id + 'has been canceled')\n",
    "        results.download_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_desc = xr.open_mfdataset('data/g0_OpenEO/descending/*.nc', decode_coords=\"all\") #automatically chuncked!\n",
    "# s1_xr_desc['VV_db'] = 10 * np.log10(s1_xr_desc['VV'])\n",
    "#s1_xr_desc['angle']  = s1_xr_desc['angle']*scale + offset   \n",
    "display(s1_xr_desc)\n",
    "xr_shape_desc = s1_xr_desc['VV'].shape\n",
    "#only full images selected\n",
    "nr_timestemps_descending = xr_shape_desc[0]\n",
    "#VV\n",
    "nr_pixels = xr_shape[1]*xr_shape[2]\n",
    "print('Number of pixels per timestamp: ' + str(nr_pixels))\n",
    "nancount = np.sum(np.isnan(s1_xr_desc['VV'].isel(t=0))).values\n",
    "print('Number of nan-pixels for a full image: ' + str(nancount))\n",
    "nan_cutoff = nancount/nr_pixels\n",
    "print('Percentage of nan-pixels in a full image: ' + str(nan_cutoff*100) + '%')\n",
    "#add 5% as safety margin to cutoff\n",
    "nan_cutoff = nan_cutoff + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_full_image = []\n",
    "for i in range(nr_timestemps_descending):\n",
    "    VV_ds = s1_xr_desc['VV'].isel(t=i)\n",
    "    temp_nancount = np.sum(np.isnan(VV_ds)).values\n",
    "    nan_frac = temp_nancount/nr_pixels\n",
    "    if nan_frac > nan_cutoff:\n",
    "        bool_full_image.append(0)\n",
    "    else:\n",
    "        bool_full_image.append(1)\n",
    "#VH\n",
    "bool_full_image_VH = []\n",
    "for i in range(nr_timestemps_descending):\n",
    "    VV_ds = s1_xr_desc['VH'].isel(t=i)\n",
    "    temp_nancount = np.sum(np.isnan(VV_ds)).values\n",
    "    nan_frac = temp_nancount/nr_pixels\n",
    "    if nan_frac > nan_cutoff:\n",
    "        bool_full_image_VH.append(0)\n",
    "    else:\n",
    "        bool_full_image_VH.append(1)\n",
    "if not bool_full_image_VH == bool_full_image:\n",
    "    pos_emp_VH = np.where(np.array(bool_full_image_VH) == 0)[0].tolist()\n",
    "    bool_full_image_np = np.array(bool_full_image)\n",
    "    bool_full_image_np[pos_emp_VH] = 0\n",
    "    bool_full_image_all = bool_full_image_np.tolist()\n",
    "    print('VH and VV Nans were not equal')\n",
    "pos_full = np.where(bool_full_image_all)[0].tolist()\n",
    "#add orbit direction\n",
    "da = xr.DataArray(\n",
    "    data = np.repeat('descending',nr_timestemps_descending),\n",
    "    dims = ['t'],\n",
    "    coords = dict(t = s1_xr_desc['t'].values)\n",
    ")\n",
    "s1_xr_desc['Orbitdirection'] = da\n",
    "s1_xr_desc_full = s1_xr_desc.isel(t= pos_full)\n",
    "display(s1_xr_desc_full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_xr_full = xr.merge([s1_xr_asc_full, s1_xr_desc_full])\n",
    "display(s1_xr_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_out:\n",
    "    s1_xr_full.to_netcdf('data/g0_OpenEO/g0_zwalm.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "a6b8480143e45034b950661dc46ed3131c1d39c9fcb21ab7eff1dd297a31067d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

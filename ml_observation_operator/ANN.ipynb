{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal ANN for observation operator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pad = Path(os.getcwd())\n",
    "if pad.name == \"ml_observation_operator\":\n",
    "    pad_correct = pad.parent\n",
    "    os.chdir(pad_correct)\n",
    "%run \"ml_observation_operator/data_load_in.py\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_data_pad = Path(\"data/Zwalm_data/ML_data\")\n",
    "X_train_all = pd.read_pickle(ML_data_pad/\"X_train.pkl\")\n",
    "X_test_all = pd.read_pickle(ML_data_pad/\"X_test.pkl\")\n",
    "X_full_all = pd.read_pickle(ML_data_pad/\"X_full.pkl\")\n",
    "\n",
    "X_train = pd.read_pickle(ML_data_pad/\"X_train.pkl\")\n",
    "X_test = pd.read_pickle(ML_data_pad/\"X_test.pkl\")\n",
    "X_full = pd.read_pickle(ML_data_pad/\"X_full.pkl\")\n",
    "\n",
    "X_train_small = pd.read_pickle(ML_data_pad/\"X_train_small.pkl\")\n",
    "X_test_small = pd.read_pickle(ML_data_pad/\"X_test_small.pkl\")\n",
    "X_full_small = pd.read_pickle(ML_data_pad/\"X_full_small.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(ML_data_pad/\"y_train.pkl\")\n",
    "y_test = pd.read_pickle(ML_data_pad/\"y_test.pkl\")\n",
    "y_full = pd.read_pickle(ML_data_pad/\"y_full.pkl\")\n",
    "\n",
    "Cstar = pd.read_pickle(ML_data_pad/\"Cstar.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop deltat_t! not needed since 1 timestep at a time considered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('delta_t',axis = 1)\n",
    "X_test = X_test.drop('delta_t',axis = 1)\n",
    "X_full = X_full.drop('delta_t',axis = 1)\n",
    "display(X_full.head())\n",
    "\n",
    "X_train_small = X_train_small.drop('delta_t',axis = 1)\n",
    "X_test_small = X_test_small.drop('delta_t',axis = 1)\n",
    "X_full_small = X_full_small.drop('delta_t',axis = 1)\n",
    "display(X_full_small.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import pickle\n",
    "import itertools\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (models,layers)\n",
    "from sklearn.metrics import r2_score\n",
    " #experiment\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from functions.plotting_functions import ensemble_plot\n",
    "from functions.ml_utils import (general_tensorflow_model, validation_loop,\n",
    "                                full_training_loop)\n",
    "SEED =1234\n",
    "exec_training = False\n",
    "#os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "#os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "python_random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: load in dask, tip to set proccesse to False seems to stop kernel from crashing! (gotten from https://e-marshall.github.io/sentinel1_rtc/asf_local_mf.html, who also had crashing kernels!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes = False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate Linear Regression in tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/75759597/typeerror-isinstance-arg-2-must-be-a-type-or-tuple-of-types-in-tensorflow-add why typeguard specification is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_full.shape[1]\n",
    "print(\"Number of features: \" + str(n_features))\n",
    "lin_reg = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (n_features,)),\n",
    "    #layers.Normalization(),\n",
    "    layers.Dense(1, activation = \"linear\", name = 'layer1'), #first number = dimensionality of the output space!\n",
    "    ]\n",
    ")\n",
    "lin_reg.summary()\n",
    "epochs = 200\n",
    "out_dict = general_tensorflow_model(lin_reg, X_train.values, X_test.values, y_train.values,\n",
    "                                     y_test.values, X_train.index, X_test.index, Cstar,\n",
    "                                    epochs = epochs, validation_split= 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the repeats: first determine optimal number of epochs on 20% hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X_train.shape[0]\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "out_list = []\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/ann/linear_regression')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "    lin_reg, repeats, X_train, X_test, y_train, y_test, Cstar,\n",
    "    exec_training, pad, epochs = epochs, dask_bool = True,\n",
    "    print_output = False, verbose = 0, validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for optimum number of epochs on full training dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " lin_reg, repeats,X_train, X_test,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split = 0.0, dask_bool = False\n",
    ")                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ANN model\n",
    "\n",
    "https://github.com/keras-team/keras/issues/597 validation_split argument in `.fit()` does not shuffle the dataset! \n",
    "\n",
    "## Full dataset\n",
    "\n",
    "Following the recommendataions as given in [understanding-deep-learning-chitta-ranjan-2021b](https://drive.google.com/file/d/1fejSMGPIDMO4eilsIDj-7QcFsCge4vdv/view)\n",
    "\n",
    "use 2 hidden layers as default\n",
    "- 1st layer: #nodes = 1/2 of of input nodes \n",
    "- 2nd layer: #nodes = 1/2 of number of nodes in previous layer\n",
    "\n",
    "relu as default activation and no dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exec_training = True\n",
    "ann = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (n_features,)),\n",
    "    layers.Dense(6, activation = \"relu\", name = 'layer1'), #first number = dimensionality of the output space!\n",
    "    layers.Dense(3, activation = \"relu\", name = 'layer2'),\n",
    "    layers.Dense(1, activation = \"linear\", name = 'layer3'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "epochs = 100\n",
    "out_list = []\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/ann/simple_ann')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        ann, repeats, X_train, X_test,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = False,\n",
    "        verbose = 0, validation_split = 0.2, dask_bool = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_r2_val);print(recommended_nr_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for the optimal number of epochs on full training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " ann, repeats,X_train, X_test,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0, dask_bool = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = ensemble_plot(Cstar, X_full.index, out_dict)\n",
    "ax.legend(loc = 'lower left')\n",
    "figpad = Path('Figures/Figures_chapter_ML_obs_op')\n",
    "if not os.path.exists(figpad):\n",
    "    os.makedirs(figpad)\n",
    "fig.savefig(figpad/'ANN_small_obs.pdf', format = 'pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower than linear regression :/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')\n",
    "print(out_dict['r2_test_list'])\n",
    "print(np.min(out_dict['r2_test_list']))\n",
    "print(np.max(out_dict['r2_test_list']))\n",
    "\n",
    "print('train')\n",
    "print(out_dict['r2_train_list'])\n",
    "print(np.min(out_dict['r2_train_list']))\n",
    "print(np.max(out_dict['r2_train_list']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with scitkitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_sklearn = MLPRegressor(hidden_layer_sizes=(2,1),solver='lbfgs',random_state = SEED)\n",
    "from functions.ml_utils import general_sklearn_model\n",
    "general_sklearn_model(ann_sklearn, X_train.values, X_test.values, y_train.values.reshape(-1,1),\n",
    "                       y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly a much better optimizer for the current state!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limited dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "epochs = 100\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/ann/simple_ann_small_data')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "n_features = X_train_small.shape[1]\n",
    "ann_small = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (n_features,)),\n",
    "    layers.Dense(6, activation = \"relu\", name = 'layer1'), #first number = dimensionality of the output space!\n",
    "    layers.Dense(3, activation = \"relu\", name = 'layer2'),\n",
    "    layers.Dense(1, activation = \"linear\", name = 'layer3'),\n",
    "    ]\n",
    ")\n",
    "ann_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        ann_small, repeats, X_train_small, X_test_small,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = False, dask_bool = False,\n",
    "        verbose = 0, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_r2_val); print(recommended_nr_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " ann_small, repeats,X_train_small, X_test_small,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0, dask_bool = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_std = 1\n",
    "ensemble_plot(Cstar, X_full.index, out_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More extended MLP model\n",
    "\n",
    "## Full dataset\n",
    "\n",
    "Recommendation of Hans: 2n+1 as number of nodes\n",
    "Add some dropout to prevent overfitting (0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "print(2*n_features+1) #round of to 24 for first layer\n",
    "ann_extended = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (n_features,)),\n",
    "    layers.Dense(24, activation = \"relu\", name = 'layer1'), #first number = dimensionality of the output space!\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(12, activation = \"relu\", name = 'layer2'),\n",
    "    layers.Dropout(0.2),    \n",
    "    layers.Dense(1, activation = \"linear\", name = 'layer3'),\n",
    "    ]\n",
    ")\n",
    "pad = Path('data/ml_obs_op_data/ann/extende_ann')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "ann_extended.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        ann_extended, repeats, X_train, X_test,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = False,\n",
    "        verbose = 0, validation_split = 0.2, dask_bool = False)\n",
    "print(mean_r2_val); print(recommended_nr_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " ann_extended, repeats,X_train, X_test,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_std = 1\n",
    "ensemble_plot(Cstar, X_full.index, out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')\n",
    "print(out_dict['r2_test_list'])\n",
    "print(np.min(out_dict['r2_test_list']))\n",
    "print(np.max(out_dict['r2_test_list']))\n",
    "\n",
    "print('train')\n",
    "print(out_dict['r2_train_list'])\n",
    "print(np.min(out_dict['r2_train_list']))\n",
    "print(np.max(out_dict['r2_train_list']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_sklearn = MLPRegressor(hidden_layer_sizes=(32,16),solver='lbfgs')\n",
    "# general_sklearn_model(ann_sklearn, X_train.values, X_test.values, y_train.values.reshape(-1,1),\n",
    "#                        y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limited dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_small  =  X_train_small.shape[1]\n",
    "print(n_features_small*2+1)\n",
    "ann_extended_small = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (n_features_small,)),\n",
    "    layers.Dense(12, activation = \"relu\", name = 'layer1'), #first number = dimensionality of the output space!\n",
    "    layers.Dense(6, activation = \"relu\", name = 'layer2'),\n",
    "    layers.Dense(1, activation = \"linear\", name = 'layer3'),\n",
    "    ]\n",
    ")\n",
    "pad = Path('data/ml_obs_op_data/ann/extended_ann_small_data')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "ann_extended.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_extended_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        ann_extended_small, repeats, X_train_small, X_test_small,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = True,\n",
    "        verbose = 0, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " ann_extended_small, repeats,X_train_small, X_test_small,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complex model structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not repeated here, was not a great experiment... see OLD notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "Executed analogous to that of LSTM. Since now no changing input sequence, a 20% validation fraction can be used! Parameters to try are:\n",
    "- number of layers: 2 or 3\n",
    "- number of nodes per layer: 4,12,20,28\n",
    "- dropout rate: 0, 0.2, 0.4 (uniform for all layers and between\n",
    "each layer)\n",
    "- large or small dataset\n",
    "\n",
    "(not checking for time drop or not, limit computation)\n",
    "\n",
    "Limit to 4 repeats per number of units! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = Path('data/ml_obs_op_data/ann/hyperparam_tuning')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append = True\n",
    "\n",
    "max_epochs = 100 #max number to try (is the default in general_tensorflow_model)\n",
    "validation_split = 0.2\n",
    "repeats = 4 #number of times to repeat training (idea of limiting computational load)\n",
    "range_nodes = np.arange(4,33,8)\n",
    "range_nr_layers = [2,3]\n",
    "# range_forest = [True, False]\n",
    "# range_time = [True, False]\n",
    "range_size = ['large','small']\n",
    "range_learning_rate = [1e-3] #as in Kratzerts\n",
    "range_dropout_rate = [0,0.2,0.4] \n",
    "n_train = X_train.shape[0]\n",
    "max_nr_options = sum([len(range_nodes)**layer for layer in range_nr_layers])*len(\n",
    "    range_learning_rate)*len(range_dropout_rate)*len(range_size)\n",
    "print(f\"Total Number of combinations {max_nr_options}\") \n",
    "col_names = ['data_in_size','nodes_1','nodes_2','nodes_3','learning_rate',\n",
    "             'drop_out_rate','R2_val','recommended_nr_epochs']\n",
    "if append:\n",
    "     pd_hyperparam_val = pd.read_csv(pad/'ann_hyperparam_validation.csv')\n",
    "else:\n",
    "    pd_hyperparam_val = pd.DataFrame(columns=col_names, index = range(0,max_nr_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_bool = False #slower but safer...\n",
    "iter = 0\n",
    "if exec_training:\n",
    "    for size_set in range_size:\n",
    "        if size_set == 'large':\n",
    "            X_temp_train = X_train.copy()\n",
    "            X_temp_test = X_test.copy()\n",
    "        elif size_set == 'small':\n",
    "            X_temp_train = X_train_small.copy()\n",
    "            X_temp_test = X_test_small.copy()\n",
    "        else:\n",
    "            raise ValueError('size_set shoulde be large or small')\n",
    "        n_features = X_temp_train.shape[1]\n",
    "        for nr_layers in range_nr_layers:\n",
    "            if nr_layers == 2:\n",
    "                network_combos = itertools.product(\n",
    "                    range_nodes, range_nodes, range_dropout_rate,\n",
    "                    range_learning_rate\n",
    "                )\n",
    "                for network_combo in network_combos:\n",
    "                    print(network_combo)\n",
    "                    (nodes_1, nodes_2, dropout_rate, \n",
    "                     learning_rate) = network_combo\n",
    "                    ann = models.Sequential(\n",
    "                        [\n",
    "                        layers.Input(shape = (n_features,)),\n",
    "                        layers.Dense(nodes_1, activation = \"relu\", name = 'layer1'),\n",
    "                        layers.Dropout(dropout_rate),\n",
    "                        layers.Dense(nodes_2, activation = \"relu\", name = 'layer2'),\n",
    "                        layers.Dropout(dropout_rate),\n",
    "                        layers.Dense(1, activation = \"linear\", name = 'layer3'),\n",
    "                        ]\n",
    "                    )\n",
    "                    ann.summary()\n",
    "                    if append:\n",
    "                        if pd_hyperparam_val.iloc[iter,:].isna().all():\n",
    "                            mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "                                ann, repeats, X_temp_train, X_temp_test, y_train, y_test,\n",
    "                                Cstar, exec_training, pad, validation_split= validation_split,\n",
    "                                learning_rate = learning_rate, verbose = 0, print_output = False,\n",
    "                                dask_bool = dask_bool\n",
    "                            )\n",
    "                            print('validation executed')\n",
    "                            pd_hyperparam_val.iloc[iter,:] = [size_set, nodes_1, nodes_2, np.nan,\n",
    "                            learning_rate, dropout_rate, mean_r2_val, recommended_nr_epochs]\n",
    "                    else:\n",
    "                        mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "                            ann, repeats, X_temp_train, X_temp_test, y_train, y_test,\n",
    "                            Cstar, exec_training, pad, validation_split= validation_split,\n",
    "                            verbose = 0, print_output = False, dask_bool = dask_bool\n",
    "                        )\n",
    "                        pd_hyperparam_val.iloc[iter,:] = [size_set, nodes_1, nodes_2, np.nan,\n",
    "                            learning_rate, dropout_rate, mean_r2_val, recommended_nr_epochs]\n",
    "                    iter = iter  + 1 \n",
    "                    pd_hyperparam_val.to_csv(pad/'ann_hyperparam_validation.csv', index = False)\n",
    "                    print(iter)\n",
    "\n",
    "            elif nr_layers == 3:\n",
    "                network_combos = itertools.product(\n",
    "                    range_nodes, range_nodes, range_nodes, range_dropout_rate,\n",
    "                    range_learning_rate\n",
    "                )\n",
    "                for network_combo in network_combos:\n",
    "                    print(network_combo)\n",
    "                    (nodes_1, nodes_2, nodes_3, dropout_rate,\n",
    "                     learning_rate) = network_combo\n",
    "                    ann = models.Sequential(\n",
    "                        [\n",
    "                        layers.Input(shape = (n_features,)),\n",
    "                        layers.Dense(nodes_1, activation = \"relu\", name = 'layer1'),\n",
    "                        layers.Dropout(dropout_rate),\n",
    "                        layers.Dense(nodes_2, activation = \"relu\", name = 'layer2'),\n",
    "                        layers.Dropout(dropout_rate),\n",
    "                        layers.Dense(nodes_3, activation = \"relu\", name ='layer3'),\n",
    "                        layers.Dropout(dropout_rate),\n",
    "                        layers.Dense(1, activation = \"linear\", name = 'layer4'),\n",
    "                        ]\n",
    "                    )\n",
    "                    ann.summary()\n",
    "                    if append:\n",
    "                        if pd_hyperparam_val.iloc[iter,:].isna().all():\n",
    "                            mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "                                ann, repeats, X_temp_train, X_temp_test, y_train, y_test,\n",
    "                                Cstar, exec_training, pad, validation_split= validation_split,\n",
    "                                learning_rate=learning_rate, verbose = 0, print_output = False,\n",
    "                                dask_bool = dask_bool\n",
    "                            )\n",
    "                            pd_hyperparam_val.iloc[iter,:] = [size_set, nodes_1, nodes_2, nodes_3,\n",
    "                            learning_rate, dropout_rate, mean_r2_val, recommended_nr_epochs]\n",
    "                    else:\n",
    "                        mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "                            ann, repeats, X_temp_train, X_temp_test, y_train, y_test,\n",
    "                            Cstar, exec_training, pad, validation_split= validation_split,\n",
    "                            verbose = 0, print_output = False, dask_bool= dask_bool\n",
    "                        )\n",
    "                        pd_hyperparam_val.iloc[iter,:] = [size_set, nodes_1, nodes_2, nodes_3,\n",
    "                            learning_rate, dropout_rate, mean_r2_val, recommended_nr_epochs]\n",
    "\n",
    "                    iter = iter  + 1\n",
    "                    pd_hyperparam_val.to_csv(pad/'ann_hyperparam_validation.csv', index = False)\n",
    "                    print(iter)\n",
    "else:\n",
    "    pd_hyperparam_val = pd.read_csv(pad/'ann_hyperparam_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_val_sorted = pd_hyperparam_val.sort_values('R2_val',ascending=False)\n",
    "pd_hyperparam_val_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_val_sorted.iloc[0,:]  #experimented with the 3 index, this one acutally performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(size_set, nodes_1, nodes_2, nodes_3, learning_rate, dropout_rate,\n",
    " r2_val, recommended_nr_epochs) = pd_hyperparam_val_sorted.iloc[0,:] \n",
    "nodes_1 = int(nodes_1)\n",
    "nodes_2 = int(nodes_2)\n",
    "recommended_nr_epochs = int(recommended_nr_epochs)\n",
    "repeats = 8\n",
    "if size_set == 'large':\n",
    "    X_train_temp = X_train\n",
    "    X_test_temp = X_test\n",
    "    n_features = X_train_temp.shape[1]\n",
    "elif size_set == 'small':\n",
    "    X_train_temp = X_train_small\n",
    "    X_test_temp = X_test\n",
    "    n_features = X_train_temp.shape[1]\n",
    "else:\n",
    "    raise ValueError('size_set must be small or large')\n",
    "if np.isnan(nodes_3):\n",
    "    ann = models.Sequential(\n",
    "        [\n",
    "        layers.Input(shape = (n_features,)),\n",
    "        layers.Dense(nodes_1, activation = \"relu\", name = 'layer1'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(nodes_2, activation = \"relu\", name = 'layer2'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(1, activation = \"linear\", name = 'layer3'),\n",
    "        ]\n",
    "    )\n",
    "else:  \n",
    "    nodes_3 = int(nodes_3)\n",
    "    ann = models.Sequential(\n",
    "        [\n",
    "        layers.Input(shape = (n_features,)),\n",
    "        layers.Dense(nodes_1, activation = \"relu\", name = 'layer1'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(nodes_2, activation = \"relu\", name = 'layer2'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(nodes_3, activation = \"relu\", name ='layer3'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(1, activation = \"linear\", name = 'layer4'),\n",
    "        ]\n",
    "    )\n",
    "ann.summary()\n",
    "mean_r2_train, mean_r2_test, out_dict, models_list =full_training_loop(ann, repeats,\n",
    "    X_train_temp, X_test_temp,y_train,y_test,Cstar, exec_training,\n",
    "    recommended_nr_epochs,pad,learning_rate = learning_rate, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_nr_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')\n",
    "print(out_dict['r2_test_list'])\n",
    "print(np.min(out_dict['r2_test_list']))\n",
    "print(np.max(out_dict['r2_test_list']))\n",
    "\n",
    "print('train')\n",
    "print(out_dict['r2_train_list'])\n",
    "print(np.min(out_dict['r2_train_list']))\n",
    "print(np.max(out_dict['r2_train_list']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt\n",
    "# def build_model(hp):\n",
    "#     \"\"\"\n",
    "#     Builds model and sets up hyperparameter space to search.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     hp : HyperParameter object\n",
    "#         Configures hyperparameters to tune.\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     model : keras model\n",
    "#         Compiled model with hyperparameters to tune.\n",
    "#     \"\"\"\n",
    "#     # Initialize sequential API and start building model.\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Input(shape = (n_features,)))\n",
    "    \n",
    "#     # Number of Units: 4 - 28 with stepsize of 8\n",
    "#     #add a first layer\n",
    "#     model.add(layers.Dense(\n",
    "#         units = hp.Int(\"units_1\", min_value =4, max_value = 28, step = 8),\n",
    "#         activation='relu'\n",
    "#     ))\n",
    "#     # Tune dropout layer with values from 0 - 0.4 with stepsize of 0.2\n",
    "#     dropout_rate = hp.Float(\"dropout\", 0, 0.4, step=0.2)\n",
    "#     model.add(layers.Dropout(dropout_rate))\n",
    "#     for i in range(hp.Int(\"num_layers\", 2, 4)): \n",
    "#         model.add(\n",
    "#             layers.Dense(\n",
    "#                 units=hp.Int(\"units_\" + str(i), min_value=4, max_value=28, step=8),\n",
    "#                 activation=\"relu\")\n",
    "#             )\n",
    "#         model.add(\n",
    "#             layers.Dropout(dropout_rate)\n",
    "#         )\n",
    "#     # Add output layer.\n",
    "#     model.add(layers.Dense(units=1, activation='linear'))\n",
    "    \n",
    "#     # Tune learning rate for Adam optimizer with values from 0.01, 0.001, or 0.0001\n",
    "#     #hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "#     # Define optimizer, loss, and metrics\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(),#learning_rate=hp_learning_rate),\n",
    "#                   loss=keras.losses.MeanSquaredError(),\n",
    "#                   metrics = tfa.metrics.RSquare()\n",
    "#                   )\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = kt.GridSearch(build_model,\n",
    "#                       objective= kt.Objective(\"val_r_square\", direction=\"max\"),\n",
    "#                       project_name = 'data/ml_obs_op_data/ann/GridSearch_full',\n",
    "#                       executions_per_trial = repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search(X_train, y_train, epochs = 100, batch_size = 32,\n",
    "#                validation_split = validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a for a in range(2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

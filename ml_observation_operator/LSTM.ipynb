{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model for observation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "pad = Path(os.getcwd())\n",
    "if pad.name == \"ml_observation_operator\":\n",
    "    pad_correct = pad.parent\n",
    "    os.chdir(pad_correct)\n",
    "%run \"ml_observation_operator/data_load_in.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import pickle\n",
    "import dask\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "from tensorflow import keras\n",
    "from keras import models, layers\n",
    "from functions.plotting_functions import ensemble_plot #plot_tf_history,\n",
    "#from functions.pre_processing import reshape_data, reshaped_to_train_test\n",
    "from functions.ml_utils import (general_tensorflow_model, validation_loop,\n",
    "                                full_training_loop)\n",
    "# from dask.distributed import Client\n",
    "# client = Client(n_workers = 4, threads_per_worker = 2)\n",
    "# display(client)\n",
    "#guanantee reproducability\n",
    "SEED =1234\n",
    "#os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "#os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "python_random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "exec_training = False\n",
    "\n",
    "print(os.getcwd())\n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distance between the days: give the number of days from the previous observation as an extra feature!! Will be used in LSTM trainig (cf obsidian notes)\n",
    "Choice was made from the previous and not the next as in this way, when predicting $y_t$ with $x_t$ as last input, the distance from $t-1$ to $t$ is used an not from $t$ to $t+1$\n",
    "\n",
    "This is performed in the `load_in.py` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_data_pad = Path(\"data/Zwalm_data/ML_data\")\n",
    "X_train_all = pd.read_pickle(ML_data_pad/\"X_train_all.pkl\")\n",
    "X_test_all = pd.read_pickle(ML_data_pad/\"X_test_all.pkl\")\n",
    "X_full_all = pd.read_pickle(ML_data_pad/\"X_full_all.pkl\")\n",
    "\n",
    "X_train = pd.read_pickle(ML_data_pad/\"X_train.pkl\")\n",
    "X_test = pd.read_pickle(ML_data_pad/\"X_test.pkl\")\n",
    "X_full = pd.read_pickle(ML_data_pad/\"X_full.pkl\")\n",
    "display(X_full.head())\n",
    "\n",
    "X_train_small = pd.read_pickle(ML_data_pad/\"X_train_small.pkl\")\n",
    "X_test_small = pd.read_pickle(ML_data_pad/\"X_test_small.pkl\")\n",
    "X_full_small = pd.read_pickle(ML_data_pad/\"X_full_small.pkl\")\n",
    "display(X_full_small.head())\n",
    "\n",
    "y_train = pd.read_pickle(ML_data_pad/\"y_train.pkl\")\n",
    "y_test = pd.read_pickle(ML_data_pad/\"y_test.pkl\")\n",
    "y_full = pd.read_pickle(ML_data_pad/\"y_full.pkl\")\n",
    "\n",
    "Cstar = pd.read_pickle(ML_data_pad/\"Cstar.pkl\")\n",
    "display(Cstar.head())\n",
    "display(X_full['delta_t'].hvplot())\n",
    "average_deltat = X_full['delta_t'].mean()\n",
    "print(f'Average delta t$ between observations: {average_deltat} days')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really regular timeseries...\n",
    "\n",
    "https://stats.stackexchange.com/questions/312609/rnn-for-irregular-time-intervals\n",
    "\n",
    "Note that this revisit time of 3 days is to be expected according to https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-1-sar/revisit-and-coverage: *A single SENTINEL-1 satellite is potentially able to map the global landmasses in the Interferometric Wide swath mode once every 12 days, in a single pass (ascending or descending). The two-satellite constellation offers a 6 day exact repeat cycle at the equator. Since the orbit track spacing varies with latitude, the revisit rate is significantly greater at higher latitudes than at the equator.*\n",
    "\n",
    "Full set of possible features: Forest, Pasture, Agriculture and a combination of pasture and agriculture\n",
    "\n",
    "Normalising and reshaping of the data is executed in the `general_tensorflow_model` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial LSTM model attempt\n",
    "A very simple LSTM model: size of hidden vector state = 10\n",
    "followed by dense layer (with no dropout between)\n",
    "Sequence length of 100 as first attempt. At an average of 3.604 days per year this is about a year of data! (analogous to idea of Kratzert 2018).\n",
    "\n",
    "Trained 8 times, average validation and training R2 determined. Then trained on all data for optimal number of epochs (max 100 epochs) based on validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "n_hidden_units = 10\n",
    "n_features = X_train.shape[1]\n",
    "lstm_model = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (seq_length, n_features)),\n",
    "    layers.LSTM(n_hidden_units),\n",
    "    layers.Dense(1, activation = 'linear')\n",
    "    ]\n",
    ")\n",
    "lstm_model.summary()\n",
    "\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/lstm/lstm_intial')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "epochs = 100\n",
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        lstm_model, repeats, X_train, X_test,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = False,\n",
    "        verbose = 0, validation_split = 0.2, lstm = True,\n",
    "        seq_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the code for 8 iterations and recommended number of epochs on the full set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " lstm_model, repeats,X_train, X_test,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0, lstm = True,\n",
    "    seq_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (9,7))\n",
    "Cstar[X_full.index].plot(ax =ax)\n",
    "ax.plot(out_dict['t_train'], out_dict['y_train_hat'], label = 'Train', alpha = 0.5, linestyle = '-')\n",
    "ax.plot(out_dict['t_test'], out_dict['y_test_hat'], label = 'Test',alpha = 0.5, linestyle = ':')\n",
    "ax.legend()\n",
    "ax.set_ylabel('C* [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an ensmeble Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict, n_std= 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the saved outputs and the output of the saved model are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nr = 6\n",
    "test_model = models_list[model_nr]#out_dict['dask_out'][model_nr]['model']#models.load_model(pad/'lstm_initial_0')\n",
    "test_model.summary()\n",
    "result = general_tensorflow_model(\n",
    "            test_model,X_train.values, X_test.values, y_train.values, \n",
    "            y_test.values, X_train.index, X_test.index, Cstar, lstm = True,\n",
    "            seq_length = seq_length, epochs = recommended_nr_epochs, print_output = False,\n",
    "            verbose = 0, validation_split = 0.0, training=False\n",
    "        )\n",
    "plt.plot(result['t_train'],result['y_train_hat'])\n",
    "plt.plot(out_dict['t_train'],out_dict['y_train_hat'][:,model_nr])\n",
    "#check equivalence with earlier calculated!\n",
    "print(np.allclose(out_dict['y_train_hat'][:,model_nr].flatten(), result['y_train_hat'].flatten()))\n",
    "plot_pd = pd.DataFrame({\n",
    "    'y_train_hat_retrieved_model':result['y_train_hat'].flatten(),\n",
    "    'y_train_hat_OG':out_dict['y_train_hat'][:,model_nr].flatten(),\n",
    "    't':result['t_train'].flatten()\n",
    "})\n",
    "plot_pd = plot_pd.set_index('t')\n",
    "display(plot_pd.hvplot())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model of Kratzert 2018\n",
    "\n",
    "Number of internal units from 20 to 10 since twice the amount of feautures used here (10 instaed of 5). Default 20% validation dataset. 365 days is about 100 timesteps for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "print(seq_length)\n",
    "n_hidden_units = 10\n",
    "kratzert_lstm = models.Sequential([\n",
    "    layers.Input((seq_length, n_features)),\n",
    "    layers.LSTM(units = n_hidden_units, name = 'lstm_1', return_sequences = True),\n",
    "    layers.Dropout(rate =0.1),\n",
    "    layers.LSTM(units = n_hidden_units, name = 'lstm_2'),\n",
    "    layers.Dropout(rate =0.1),\n",
    "    layers.Dense(units =1, activation = 'linear')\n",
    "])\n",
    "kratzert_lstm.summary()\n",
    "\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/lstm/lstm_kratzert')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "epochs = 100\n",
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        kratzert_lstm, repeats, X_train, X_test,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = False,\n",
    "        verbose = 0, validation_split = 0.2, lstm = True,\n",
    "        seq_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again repeat on full dataset with optimal number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " kratzert_lstm, repeats,X_train, X_test,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0, lstm = True,\n",
    "    seq_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Aveage R2 on training set: {mean_r2_train}')\n",
    "print(f'Average R2 on test set: {mean_r2_test}')\n",
    "ensemble_plot(Cstar, X_full.index, out_dict, n_std = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly an even more overfitted version! Indicates the seeming importance of regularisation or reducing the amount of parameters to be fitted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "New idea: to limit computational burden just do a fixed number of validation Cstar to model!\n",
    "\n",
    "- 1 layer of LSTM: deemed best by Kratzert (large sample hydrology), prevent overfitting \n",
    "- dropout between LSTM and last dense layer (analogous to Kratzert): 0, 0.2 or 0.4\n",
    "- keep recurrent dropout to 0 \n",
    "- hidden unit range:4,8,12,16\n",
    "- choose between large and small dataset!\n",
    "- keep or drop: forest feature, time feature\n",
    "- keep deault learning rate of 1e-3 + experiment with 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.config.set({'distributed.scheduler.default-task-durations': '1h'})\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/lstm/hyperparam_tuning')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "\n",
    "append = True\n",
    "\n",
    "max_epochs = 100 #max number to try\n",
    "n_val = 100 #irrespective of input sequence length, keep the same last 100 sequences as test data!\n",
    "repeats = 4 #number of times to repeat training (idea of limiting computational load)\n",
    "n_train = X_train.shape[0]\n",
    "range_forest = [True, False]\n",
    "range_time = [True, False]\n",
    "range_size = ['large','small']\n",
    "range_hidden_unit = np.arange(4,18,4)\n",
    "range_input_seq = np.array([30,60,90])\n",
    "range_learning_rate = [1e-3] #as in Kratzert\n",
    "range_dropout_rate = [0,0.2,0.4] #dropout on the last output cell \n",
    "max_nr_options = len(range_time)*len(range_forest)*len(\n",
    "    range_input_seq)*len(range_learning_rate)*len(\n",
    "    range_hidden_unit)*len(range_dropout_rate)*len(range_size)\n",
    "print(f\"Total Number of combinations {max_nr_options}\") \n",
    "col_names = ['data_in_size','seq_length','forest_bool','time_bool','hidden_units','learning_rate',\n",
    "             'drop_out_rate','R2_val','recommended_nr_epochs']\n",
    "if append:\n",
    "     pd_hyperparam_val = pd.read_csv(pad/'lstm_hyperparam_validation.csv')\n",
    "else:\n",
    "    pd_hyperparam_val = pd.DataFrame(columns=col_names, index = range(0,max_nr_options))\n",
    "iter = 0\n",
    "input_full_combos= itertools.product(range_input_seq, range_forest, range_time)\n",
    "input_small_combos = itertools.product(range_input_seq, range_time)\n",
    "network_combos = itertools.product(range_hidden_unit, range_learning_rate, range_dropout_rate)\n",
    "\n",
    "#the reoccuring loop\n",
    "def lstm_repeating(X_temp_train, X_temp_test, network_combo, seq_length, n_features, iter):\n",
    "        hidden_units, learning_rate, dropout_rate = network_combo\n",
    "        model_name = 'lstm_' + str(iter)\n",
    "        #out_list = []\n",
    "        r2_list = []\n",
    "        best_epoch_list = []\n",
    "        for i in range(repeats):\n",
    "            lstm_model = models.Sequential(\n",
    "                [\n",
    "                layers.Input(shape = (seq_length, n_features)),\n",
    "                layers.LSTM(hidden_units),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(1, activation ='linear')\n",
    "                ],\n",
    "                name = model_name\n",
    "            )\n",
    "            result = general_tensorflow_model(\n",
    "                lstm_model,X_temp_train.values, X_temp_test.values, y_train.values, \n",
    "                y_test.values, X_train.index, X_test.index, Cstar, lstm = True,\n",
    "                seq_length = seq_length, epochs = max_epochs, print_output = False,\n",
    "                verbose = 0, validation_split = n_val, learning_rate = learning_rate\n",
    "            )\n",
    "            r2_list.append(result['max_val_R2'])\n",
    "            best_epoch_list.append(result['best_epoch'])\n",
    "        mean_r2_val = np.mean(r2_list)\n",
    "        recommended_nr_epochs = int(np.round(np.mean(best_epoch_list)))\n",
    "        print(f'Mean R2 on validation set {mean_r2_val}')\n",
    "        print(f'Recommended number of epochs: {recommended_nr_epochs}')\n",
    "        pd_hyperparam_val.iloc[iter,:] = [size_set, seq_length, forest_bool, time_bool, hidden_units,\n",
    "            learning_rate, dropout_rate, mean_r2_val, recommended_nr_epochs]\n",
    "        return iter, pd_hyperparam_val\n",
    "\n",
    "#all the options \n",
    "if exec_training:\n",
    "    for size_set in range_size:\n",
    "        if size_set == 'large':\n",
    "            for input_combo in input_full_combos:\n",
    "                print(input_combo)\n",
    "                seq_length, forest_bool, time_bool = input_combo\n",
    "                X_temp_train = X_train.copy()\n",
    "                X_temp_test = X_test.copy()\n",
    "                if not time_bool:\n",
    "                    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "                    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)   \n",
    "                if not forest_bool:\n",
    "                    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')] \n",
    "                    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')] \n",
    "                n_features = X_temp_train.shape[1]\n",
    "                network_combos = itertools.product(range_hidden_unit, range_learning_rate, range_dropout_rate)\n",
    "                for network_combo in network_combos:\n",
    "                    print(network_combo)\n",
    "                    if append:\n",
    "                        if pd_hyperparam_val.iloc[iter,:].isna().all():\n",
    "                            iter, pd_hyperparam_val = lstm_repeating(\n",
    "                            X_temp_train, X_temp_test, network_combo,\n",
    "                            seq_length, n_features, iter)\n",
    "                    else:\n",
    "                        iter, pd_hyperparam_val = lstm_repeating(\n",
    "                               X_temp_train, X_temp_test, network_combo,\n",
    "                               seq_length, n_features, iter)\n",
    "                    iter = iter + 1\n",
    "                    print(str(iter) + ' out of maximally ' + str(max_nr_options))\n",
    "                    pd_hyperparam_val.to_csv(pad/'lstm_hyperparam_validation.csv', index = False)\n",
    "        if size_set == 'small':\n",
    "            for input_combo in input_small_combos:\n",
    "                print(input_combo)\n",
    "                seq_length, time_bool = input_combo\n",
    "                X_temp_train = X_train_small.copy()\n",
    "                X_temp_test = X_test_small.copy()\n",
    "                if not time_bool:\n",
    "                    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "                    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)    \n",
    "                n_features = X_temp_train.shape[1]\n",
    "                network_combos = itertools.product(range_hidden_unit, range_learning_rate, range_dropout_rate)\n",
    "                for network_combo in network_combos:\n",
    "                    if append:\n",
    "                        if pd_hyperparam_val.iloc[iter,:].isna().all():\n",
    "                            iter, pd_hyperparam_val = lstm_repeating(\n",
    "                            X_temp_train, X_temp_test, network_combo,\n",
    "                            seq_length, n_features, iter)\n",
    "                    else:\n",
    "                        iter, pd_hyperparam_val = lstm_repeating(\n",
    "                               X_temp_train, X_temp_test, network_combo,\n",
    "                               seq_length, n_features, iter)\n",
    "                    iter = iter + 1\n",
    "                    print(str(iter) + ' out of maximally ' + str(max_nr_options))\n",
    "                    pd_hyperparam_val.to_csv(pad/'lstm_hyperparam_validation.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: only the good model structures (based on their score on the validation set) will trained using the full training set!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exec_training:\n",
    "    pd_hyperparam_val.to_csv(pad/'lstm_hyperparam_validation.csv', index = False)\n",
    "else:\n",
    "    pd_hyperparam_val = pd.read_csv(pad/'lstm_hyperparam_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_val_sorted = pd_hyperparam_val.sort_values('R2_val',ascending = False)\n",
    "pd_hyperparam_val_sorted.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so time bool ofen dropped. larger models need more dropout as regularization!\n",
    "\n",
    "Check out how the small model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_val_small = pd_hyperparam_val[pd_hyperparam_val['data_in_size']== 'small']\n",
    "pd_hyperparam_val_small_sorted = pd_hyperparam_val_small.sort_values('R2_val',ascending=False)\n",
    "pd_hyperparam_val_small_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with the highest validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd_hyperparam_val_sorted.iloc[0,:].to_list()\n",
    "print(parameters)\n",
    "size_set, seq_length, forest_bool, time_bool, hidden_units, learning_rate, dropout_rate, r2_val, recommended_nr_epochs = parameters\n",
    "if size_set == 'large':\n",
    "    X_temp_train = X_train.copy()\n",
    "    X_temp_test = X_test.copy()\n",
    "elif size_set == 'small':\n",
    "    X_temp_train = X_train_small.copy()\n",
    "    X_temp_test = X_test_small.copy()\n",
    "if not time_bool:\n",
    "    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)   \n",
    "if not forest_bool:\n",
    "    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')] \n",
    "    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')] \n",
    "n_features = X_train.shape[1]\n",
    "lstm_hyperparm = models.Sequential(\n",
    "    [\n",
    "    layers.Input((int(seq_length),n_features)),\n",
    "    layers.LSTM(int(hidden_units)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "    layers.Dense(1,'linear')    \n",
    "    ]\n",
    ")\n",
    "lstm_hyperparm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_nr_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "pad = Path('data/ml_obs_op_data/lstm/hyperparam_tuning')\n",
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " lstm_hyperparm, repeats,X_train, X_test,y_train,y_test, Cstar,\n",
    " exec_training, int(recommended_nr_epochs), pad, print_output = True,\n",
    " verbose = 0, validation_split=0.0, lstm = True,\n",
    "    seq_length = int(seq_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict, n_std = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_nr_epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with the highest validation score of the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd_hyperparam_val_small_sorted.iloc[0,:].to_list()\n",
    "print(parameters)\n",
    "size_set, seq_length, forest_bool, time_bool, hidden_units, learning_rate, dropout_rate, r2_val, recommended_nr_epochs = parameters\n",
    "if size_set == 'large':\n",
    "    X_temp_train = X_train.copy()\n",
    "    X_temp_test = X_test.copy()\n",
    "elif size_set == 'small':\n",
    "    X_temp_train = X_train_small.copy()\n",
    "    X_temp_test = X_test_small.copy()\n",
    "if not time_bool:\n",
    "    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)   \n",
    "if not forest_bool:\n",
    "    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')] \n",
    "    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')] \n",
    "n_features = X_temp_train.shape[1]\n",
    "lstm_hyperparm = models.Sequential(\n",
    "    [\n",
    "    layers.Input((int(seq_length),n_features)),\n",
    "    layers.LSTM(int(hidden_units)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "    layers.Dense(1,'linear')    \n",
    "    ]\n",
    ")\n",
    "lstm_hyperparm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "pad = Path('data/ml_obs_op_data/lstm/hyperparam_tuning/small_models')\n",
    "if not os.path.exists(pad):\n",
    "    os.mkdir(pad)\n",
    "\n",
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " lstm_hyperparm, repeats,X_train_small, X_test_small,y_train,y_test, Cstar,\n",
    " exec_training, int(recommended_nr_epochs), pad, print_output = True,\n",
    " verbose = 0, validation_split=0.0, lstm = True,\n",
    "    seq_length = int(seq_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict, n_std = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial LSTM model without time features\n",
    "\n",
    "Learning from working with the window data on Ridge Regression: you can exclude the sin wave for better performance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "n_hidden_units = 10\n",
    "n_features = X_train.shape[1]\n",
    "#drop sin and cos features\n",
    "X_train_nt = X_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "X_test_nt = X_test.drop(['year_sin','year_cos'],axis =1)\n",
    "n_features = X_train_nt.shape[1]\n",
    "lstm_model = models.Sequential(\n",
    "    [\n",
    "    layers.Input(shape = (seq_length, n_features)),\n",
    "    layers.LSTM(n_hidden_units),\n",
    "    layers.Dense(1, activation = 'linear')\n",
    "    ]\n",
    ")\n",
    "lstm_model.summary()\n",
    "\n",
    "#For later storage\n",
    "pad = Path('data/ml_obs_op_data/lstm/lstm_intial_nt')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 8\n",
    "epochs = 100\n",
    "mean_r2_val, recommended_nr_epochs = validation_loop(\n",
    "        lstm_model, repeats, X_train_nt, X_test_nt,y_train, y_test, Cstar,\n",
    "        exec_training,pad,epochs = epochs,print_output = False,\n",
    "        verbose = 0, validation_split = 0.2, lstm = True,\n",
    "        seq_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_train, mean_r2_test, out_dict, models_list = full_training_loop(\n",
    " lstm_model, repeats,X_train_nt, X_test_nt,y_train,y_test, Cstar,\n",
    " exec_training, recommended_nr_epochs, pad, print_output = False,\n",
    " verbose = 0, validation_split=0.0, lstm = True,\n",
    "    seq_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_plot(Cstar, X_full.index, out_dict,n_std = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models for the observation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the complex LSTM model, we'll first check the performance of much simpler ML models: linear regression and ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import dask\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, RidgeCV,\n",
    "                                   LassoCV, Lasso)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "pad = Path(os.getcwd())\n",
    "if pad.name == \"ml_observation_operator\":\n",
    "    pad_correct = pad.parent\n",
    "    os.chdir(pad_correct)\n",
    "from functions.PDM import PDM\n",
    "from functions.pre_processing import reshape_data, reshaped_to_train_test\n",
    "from functions.ml_utils import general_sklearn_model\n",
    "SEED = 1234\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "exec_hyperopt_tuning = True\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run \"ml_observation_operator/data_load_in.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ML_data_pad = Path(\"data/Zwalm_data/ML_data\")\n",
    "X_full_all = pd.read_pickle(ML_data_pad/\"X_full_all.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(ML_data_pad/\"y_train.pkl\")\n",
    "y_test = pd.read_pickle(ML_data_pad/\"y_test.pkl\")\n",
    "y_full = pd.read_pickle(ML_data_pad/\"y_full.pkl\")\n",
    "\n",
    "Cstar = pd.read_pickle(ML_data_pad/\"Cstar.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full set of possible features: Forest, Pasture, Agriculture and a combination of pasture and agricculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_corr = X_full_all.corr()\n",
    "features_corr.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in both full and the smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(ML_data_pad/\"X_train.pkl\")\n",
    "X_test = pd.read_pickle(ML_data_pad/\"X_test.pkl\")\n",
    "X_full = pd.read_pickle(ML_data_pad/\"X_full.pkl\")\n",
    "display(X_full.head())\n",
    "\n",
    "X_train_small = pd.read_pickle(ML_data_pad/\"X_train_small.pkl\")\n",
    "X_test_small = pd.read_pickle(ML_data_pad/\"X_test_small.pkl\")\n",
    "X_full_small = pd.read_pickle(ML_data_pad/\"X_full_small.pkl\")\n",
    "display(X_full_small.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train_all.iloc[:,~X_train_all.columns.str.endswith('past_agr')]\n",
    "X_test = X_test_all.iloc[:,~X_test_all.columns.str.endswith('past_agr')]\n",
    "X_full = X_full_all.iloc[:,~X_full_all.columns.str.endswith('past_agr')]\n",
    "\n",
    "\n",
    "X_train_small = X_train_all.iloc[:,~X_train_all.columns.str.endswith(('Forest','Pasture','Agriculture'))]\n",
    "X_test_small = X_test_all.iloc[:,~X_test_all.columns.str.endswith(('Forest','Pasture','Agriculture'))]\n",
    "X_full_small = X_full_all.iloc[:,~X_full_all.columns.str.endswith(('Forest','Pasture','Agriculture'))]\n",
    "display(X_full_small.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24/03/2023: drop delta_t feature as experiment (since this will not lead to better perfomance if not multiple timesteps included). From 0.7 to around 0.74 in test score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('delta_t',axis = 1)\n",
    "X_test = X_test.drop('delta_t',axis = 1)\n",
    "X_full = X_full.drop('delta_t',axis = 1)\n",
    "\n",
    "X_train_small = X_train_small.drop('delta_t',axis = 1)\n",
    "X_test_small = X_test_small.drop('delta_t',axis = 1)\n",
    "X_full_small = X_full_small.drop('delta_t',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea of linear regression as observation operator alreayd applied in Auber(cf. [obsidian](C:\\Users\\olivi\\Documents\\ob_obsidian\\DA\\Aubert_SM_DA_in_conceptual_model.md) and https://www.sciencedirect.com/science/article/pii/S0022169403002294?via%3Dihub )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include forest in the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2 \n",
    "\n",
    "Calculated adjusted $R^2$ (=$\\bar{R}^2$) from regular $R^2$ as:\n",
    "$$\n",
    "{\\displaystyle {\\bar {R}}^{2}=1-(1-R^{2}){n-1 \\over n-p}}\n",
    "$$\n",
    "\n",
    "with $n$ the number of variables  and $p$ the number parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 input, 1 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very basic idea: 1 input timestep to 1 output timestep without normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg, r2_train, r2_test, fig, ax = general_sklearn_model(LinearRegression(), X_train,\n",
    "                                                X_test, y_train, y_test, X_train.index,\n",
    "                                                X_test.index, Cstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add normalisation to input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_norm, r2_train, r2_test, fig, ax = general_sklearn_model(LinearRegression(), X_train,\n",
    "                                                X_test, y_train.values.reshape(-1,1), \n",
    "                                                y_test.values.reshape(-1,1), X_train.index,\n",
    "                                                  X_test.index, Cstar, normalisation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the sin and cos feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_drop, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_train.drop(['year_sin','year_cos'], axis = 1),\n",
    "     X_test.drop(['year_sin','year_cos'], axis = 1), y_train.values.reshape(-1,1),\n",
    "     y_test.values.reshape(-1,1),X_train.index, X_test.index, Cstar\n",
    ")\n",
    "                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the forest related features (but keep time related feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.index.union(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_no_forest = X_train.loc[:,~X_train.columns.str.endswith('Forest')]\n",
    "X_test_no_forest = X_test.loc[:,~X_test.columns.str.endswith('Forest')]\n",
    "linreg_drop_forest, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_train_no_forest, X_test_no_forest,\n",
    "    y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),\n",
    "    X_train.index, X_test.index, Cstar\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure how to calculate R2 adjusted on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion on normalisation: not really necessary in this case, basically no difference in performance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coef_dict =  {}\n",
    "for i, param in enumerate(X_train.columns.to_list()):\n",
    "    coef_dict[param] = linreg.coef_[i]\n",
    "pd_coef = pd.DataFrame(coef_dict, index =[0])\n",
    "pd_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a lot of weight being given to the sinus feature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now also try with the dataset with less features: only the lumped pasture and agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_small, r2_train, r2_test, fig, ax = general_sklearn_model(LinearRegression(), X_train_small,\n",
    "                                                X_test_small, y_train, y_test, X_train.index,\n",
    "                                                X_test.index, Cstar)\n",
    "#ax.plot(X_full.index, X_full['year_sin']*150+310, label = 'Sine wave')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse performance than on the more full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_small, r2_train, r2_test, fig, ax = general_sklearn_model(LinearRegression(), X_train_small,\n",
    "                                                X_test_small, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),\n",
    "                                                X_train.index, X_test.index, Cstar, normalisation= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_small, r2_train, r2_test, fig, ax = general_sklearn_model(LinearRegression(), X_train_small.drop(['year_sin','year_cos'],axis =1),\n",
    "                                                X_test_small.drop(['year_sin','year_cos'],axis =1), y_train.values.reshape(-1,1), \n",
    "                                                y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar,normalisation = True)\n",
    "ax.set_title('Normalised Linear Regression: no time features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly a lot of dependence on the sinus feature to get decent results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### several input timesteps, 1 output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 5\n",
    "X_window, y_window, t_window = reshape_data(\n",
    "    X_full.values,y_full.values.reshape(-1,1),\n",
    "    X_full.index.values, seq_length\n",
    ")\n",
    "print(X_window.shape)\n",
    "print(t_window.shape)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "(X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "    X_window, y_window, t_window, seq_length, n_train, 2\n",
    ")\n",
    "\n",
    "print(X_window_train.shape)\n",
    "print(y_window_train.shape)\n",
    "print(X_window_test.shape)\n",
    "print(y_window_test.shape)\n",
    "\n",
    "linreg_window, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_window_train, X_window_test,\n",
    "     y_window_train,\n",
    "     y_window_test, t_window_train, X_test.index, Cstar\n",
    ")\n",
    "ax.set_title(\"Window LinearRegression on max # features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So linear regression quite clearly overfits on the window data => idea of trying ridge regression\n",
    "\n",
    "Also try on the smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_window, y_window, t_window = reshape_data(\n",
    "    X_full_small.values,y_full.values.reshape(-1,1),\n",
    "    X_full_small.index.values, seq_length\n",
    ")\n",
    "n_train = X_train.shape[0]\n",
    "(X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "    X_window, y_window, t_window, seq_length, n_train, 2\n",
    ")\n",
    "linreg_window_small, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_window_train, X_window_test,\n",
    "     y_window_train,\n",
    "     y_window_test, t_window_train, X_test.index, Cstar\n",
    ")\n",
    "ax.set_title(\"Window Linear Regression on the lumped dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So worse performance by including more timesteps! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso regression\n",
    "\n",
    "L2 normalisation. Well explained in https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification \n",
    "\n",
    "Aslo L1 normalisation ncan be tried with Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html automatic best hyperparamter $\\alpha$ (often called $\\lambda$) for regularisation by appyling crosss validation. cv = 5 is 5-fold cross-validation within the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 input 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle=True) #prevent shuffling the dat\n",
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "names = [\"ridge\",\"lasso\"]\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "     ridge, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "          model, X_train, X_test, y_train,\n",
    "          y_test, X_train.index, X_test.index, Cstar\n",
    "     )\n",
    "     ax.set_title(names[i] + r' with $\\alpha = $' +  f'{model.alpha_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse than linear regression: both for Lasso and Ridge (but Lasso performs sligthly better)\n",
    "\n",
    "Include normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "     ridge_norm, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "          model, X_train.values, X_test.values, y_train.values.reshape(-1,1),\n",
    "          y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    "     )\n",
    "     ax.set_title('Normalised ' + names[i] + r'with $\\alpha = $' +  f'{model.alpha_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better performance with normalisation. Lasso normalisation has highest test score thusfar\n",
    "\n",
    "Drop time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "    ridge_drop, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "        model , X_train.drop(['year_sin','year_cos'], axis = 1),\n",
    "        X_test.drop(['year_sin','year_cos'], axis = 1), y_train.values.reshape(-1,1),\n",
    "        y_test.values.reshape(-1,1),X_train.index, X_test.index, Cstar, normalisation = True\n",
    "    )\n",
    "    ax.set_title('Normalised ' + names[i] + r' no time features with $\\alpha = $' +  f'{model.alpha_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "    linreg_drop_forest, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "        model, X_train_no_forest, X_test_no_forest,\n",
    "        y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),\n",
    "        X_train.index, X_test.index, Cstar, normalisation = True #normalisation = slightly better performance\n",
    "    )\n",
    "    ax.set_title('Normalised RidgeCV drop forest')\n",
    "    ax.set_title('Normalised ' + names[i] + r' drop forest with $\\alpha = $' +  f'{model.alpha_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple inputs, 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge_window, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     ridge, X_window_train, X_window_test,\n",
    "     y_window_train,\n",
    "     y_window_test, t_window_train, X_test.index, Cstar\n",
    ")\n",
    "ax.set_title(\"Window Ridge regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 30\n",
    "X_window, y_window, t_window = reshape_data(\n",
    "    X_full.drop(['year_sin','year_cos'],axis = 1).values,y_full.values.reshape(-1,1),\n",
    "    X_full.index.values, seq_length\n",
    ")\n",
    "print(X_window.shape)\n",
    "print(t_window.shape)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "(X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "    X_window, y_window, t_window, seq_length, n_train, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge_window_drop, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     ridge, X_window_train, X_window_test,\n",
    "     y_window_train,\n",
    "     y_window_test, t_window_train, X_test.index, Cstar\n",
    ")\n",
    "ax.set_title('Ridge window no time features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge_window_drop.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for best model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for the full dataset:\n",
    "- per model (so Ridge, Lasso and normal Linear regression) find the best set of hyperparameters:\n",
    "    - sequence length of input: 1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60\n",
    "    - forest or no forest\n",
    "    - time of no time\n",
    "    - range of alpha values (include 0 = linear regression is included!)\n",
    "Normalisation is given to inputs to avoid problems with trainig of the algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "simplefilter(\"ignore\", category=UserWarning)\n",
    "nr_folds = 4\n",
    "n_train = X_train.shape[0]\n",
    "alpha_range = np.concatenate([np.logspace(-3,3,100),np.array([0])]) \n",
    "range_seq_length = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60])#2**np.arange(1,7)#np.arange(1,100,10)\n",
    "range_forest = [True, False]\n",
    "range_time_goniometr = [True, False]\n",
    "model_names = ['Ridge', 'Lasso']\n",
    "nr_options = len(range_seq_length)*len(range_forest)*len(range_time_goniometr)*len(alpha_range)*len(model_names)\n",
    "col_names = ['model','seq_length','forest_bool','time_bool','alpha','r2_val_mean','r2_val_sd']\n",
    "pd_hyperparam = pd.DataFrame(columns=col_names, index = range(0,nr_options))\n",
    "iter = 0\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "X_full_norm = pd.DataFrame(X_scaler.transform(X_full), columns = X_full.columns)\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(y_train.values.reshape(-1,1))\n",
    "y_full_norm = y_scaler.transform(y_full.values.reshape(-1,1))\n",
    "if exec_hyperopt_tuning:\n",
    "    for seq_length in range_seq_length:\n",
    "        for forest in range_forest:\n",
    "            for time_goniometr in range_time_goniometr:\n",
    "                X_temp = X_full_norm.copy()\n",
    "                if not time_goniometr:\n",
    "                    X_temp = X_temp.drop(['year_sin','year_cos'],axis = 1)\n",
    "                if not forest:\n",
    "                    X_temp = X_temp.loc[:,~X_temp.columns.str.endswith('Forest')]\n",
    "                X_window, y_window, t_window = reshape_data(\n",
    "                    X_temp.values,y_full_norm,\n",
    "                    X_full.index.values, seq_length\n",
    "                )\n",
    "                (X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "                t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "                    X_window, y_window, t_window, seq_length, n_train, output_dim = 2\n",
    "                )\n",
    "                for alpha in alpha_range:\n",
    "                    for model_name in model_names:\n",
    "                        kf = KFold(nr_folds, shuffle = False)\n",
    "                        r2_val_list = []\n",
    "                        for i, (train_index, test_index) in enumerate(kf.split(X_window_train)):\n",
    "                            if model_name == 'Lasso':\n",
    "                                model = Lasso(alpha = alpha)\n",
    "                            else:\n",
    "                                model = Ridge(alpha = alpha)\n",
    "                            #model_temp, r2_train, r2_val, fig, ax = general_sklearn_model(\n",
    "                            delayed_result = dask.delayed(general_sklearn_model)(\n",
    "                                model, X_window_train[train_index], X_window_train[test_index],\n",
    "                                y_window_train[train_index],y_window_train[test_index], \n",
    "                                t_window_train[train_index], t_window_train[test_index], Cstar,\n",
    "                                print_output = False\n",
    "                            )\n",
    "                            r2_val_list.append(delayed_result)\n",
    "                            #r2_val_list.append(r2_val)\n",
    "                        r2_vals = dask.compute(*r2_val_list)\n",
    "                        r2_vals = [r2_vals[i][2] for i in range(len(r2_vals))]\n",
    "                        pd_hyperparam.iloc[iter,:] = [model_name,seq_length, forest, time_goniometr,\n",
    "                                                    alpha, np.mean(r2_vals),np.std(r2_vals)]\n",
    "                        iter = iter + 1 \n",
    "                        if iter%100 == 0:\n",
    "                            print(f'Iteration {iter} out of {nr_options} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exec_hyperopt_tuning:\n",
    "    pd_hyperparam.to_csv(\"data/ml_obs_op_data/lin_reg_hyperparam_cv.csv\", index = False)\n",
    "else:\n",
    "    pd_hyperparam = pd.read_csv(\"data/ml_obs_op_data/lin_rge_hyperparam_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_best = pd_hyperparam[pd_hyperparam['r2_val_mean'].max() == pd_hyperparam['r2_val_mean']]\n",
    "hyperparam_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain a model with the above information on the entire training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = hyperparam_best['alpha'].values[0]\n",
    "seq_length = hyperparam_best['seq_length'].values[0]\n",
    "time_goniometr = hyperparam_best['time_bool'].values[0]\n",
    "forest = hyperparam_best['forest_bool'].values[0]\n",
    "X_temp = X_full_norm.copy()\n",
    "if not time_goniometr:\n",
    "    X_temp = X_temp.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not forest:\n",
    "    X_temp = X_temp.loc[:,~X_temp.columns.str.endswith('Forest')]\n",
    "X_window, y_window, t_window = reshape_data(\n",
    "    X_temp.values, y_full_norm,\n",
    "    X_temp.index.values, seq_length\n",
    ")\n",
    "(X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "    X_window, y_window, t_window, seq_length, n_train, output_dim = 2\n",
    ")\n",
    "ridge = Ridge(alpha=alpha)#LOOCV as the default here (fastest)\n",
    "ridge.fit(X_window_train, y_window_train)\n",
    "y_train_hat = ridge.predict(X_window_train)\n",
    "y_test_hat = ridge.predict(X_window_test)\n",
    "y_train_hat = y_scaler.inverse_transform(y_train_hat)\n",
    "y_test_hat = y_scaler.inverse_transform(y_test_hat)\n",
    "fig, ax = plt.subplots()\n",
    "Cstar.plot(ax=ax)\n",
    "ax.plot(X_train_all.index[t_window_train], y_train_hat, label = 'Train')\n",
    "ax.plot(X_full_all.index[t_window_test], y_test_hat, label = 'Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('C* [mm]')\n",
    "r2_train = r2_score(y_window_train,y_scaler.transform(y_train_hat))\n",
    "r2_test = r2_score(y_window_test, y_scaler.transform(y_test_hat))\n",
    "print(f'trainig R2: {r2_train}')\n",
    "print(f'test R2: {r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ridge regresion even after optimisation of hyperparameters, does not perform better than a simple linear regression on al features with a window base approach. Note that in fact a simple linear regression without the window base approach outperforms this approach on test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam.sort_values('r2_val_mean',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_lasso = pd_hyperparam[pd_hyperparam['model'] == 'Lasso']\n",
    "pd_hyperparam_lasso.sort_values('r2_val_mean',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_lasso_best = pd_hyperparam_lasso[\n",
    "    pd_hyperparam_lasso['r2_val_mean'].max() == pd_hyperparam_lasso['r2_val_mean']\n",
    "]\n",
    "alpha = hyperparam_lasso_best['alpha'].values[0]\n",
    "seq_length = hyperparam_lasso_best['seq_length'].values[0]\n",
    "time_goniometr = hyperparam_lasso_best['time_bool'].values[0]\n",
    "forest = hyperparam_lasso_best['forest_bool'].values[0]\n",
    "X_temp = X_full_norm.copy()\n",
    "if not time_goniometr:\n",
    "    X_temp = X_temp.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not forest:\n",
    "    X_temp = X_temp.loc[:,~X_temp.columns.str.endswith('Forest')]\n",
    "X_window, y_window, t_window = reshape_data(\n",
    "    X_temp.values, y_full_norm,\n",
    "    X_temp.index.values, seq_length\n",
    ")\n",
    "(X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "    X_window, y_window, t_window, seq_length, n_train, output_dim = 2\n",
    ")\n",
    "lasso = Lasso(alpha=alpha)#LOOCV as the default here (fastest)\n",
    "lasso.fit(X_window_train, y_window_train)\n",
    "y_train_hat = lasso.predict(X_window_train)\n",
    "y_test_hat = lasso.predict(X_window_test)\n",
    "y_train_hat = y_scaler.inverse_transform(y_train_hat.reshape(-1,1))\n",
    "y_test_hat = y_scaler.inverse_transform(y_test_hat.reshape(-1,1))\n",
    "fig, ax = plt.subplots()\n",
    "Cstar.plot(ax=ax)\n",
    "ax.plot(X_train_all.index[t_window_train], y_train_hat, label = 'Train')\n",
    "ax.plot(X_full_all.index[t_window_test], y_test_hat, label = 'Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('C* [mm]')\n",
    "r2_train = r2_score(y_window_train,y_scaler.transform(y_train_hat))\n",
    "r2_test = r2_score(y_window_test, y_scaler.transform(y_test_hat))\n",
    "print(f'trainig R2: {r2_train}')\n",
    "print(f'test R2: {r2_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_sorted = pd_hyperparam[pd_hyperparam['time_bool'] == False].sort_values('r2_val_mean',ascending = False)\n",
    "no_time_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_best = pd.DataFrame(no_time_sorted.iloc[0,:].values.reshape(1,-1), columns = no_time_sorted.columns)\n",
    "hyperparam_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_best = pd.DataFrame(no_time_sorted.iloc[0,:].values.reshape(1,-1), columns = no_time_sorted.columns)\n",
    "alpha = hyperparam_best['alpha'].values[0]\n",
    "seq_length = hyperparam_best['seq_length'].values[0]\n",
    "time_goniometr = hyperparam_best['time_bool'].values[0]\n",
    "forest = hyperparam_best['forest_bool'].values[0]\n",
    "X_temp = X_full_norm.copy()\n",
    "if not time_goniometr:\n",
    "    X_temp = X_temp.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not forest:\n",
    "    X_temp = X_temp.loc[:,~X_temp.columns.str.endswith('Forest')]\n",
    "X_window, y_window, t_window = reshape_data(\n",
    "    X_temp.values, y_full_norm,\n",
    "    X_temp.index.values, seq_length\n",
    ")\n",
    "(X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "    X_window, y_window, t_window, seq_length, n_train, output_dim = 2\n",
    ")\n",
    "ridge = Ridge(alpha=alpha)#LOOCV as the default here (fastest)\n",
    "ridge.fit(X_window_train, y_window_train)\n",
    "y_train_hat = ridge.predict(X_window_train)\n",
    "y_test_hat = ridge.predict(X_window_test)\n",
    "y_train_hat = y_scaler.inverse_transform(y_train_hat)\n",
    "y_test_hat = y_scaler.inverse_transform(y_test_hat)\n",
    "fig, ax = plt.subplots()\n",
    "Cstar.plot(ax=ax)\n",
    "ax.plot(X_train_all.index[t_window_train], y_train_hat, label = 'Train')\n",
    "ax.plot(X_full_all.index[t_window_test], y_test_hat, label = 'Test')\n",
    "ax.legend()\n",
    "ax.set_ylabel('C* [mm]')\n",
    "r2_train = r2_score(y_window_train,y_scaler.transform(y_train_hat))\n",
    "r2_test = r2_score(y_window_test, y_scaler.transform(y_test_hat))\n",
    "print(f'trainig R2: {r2_train}')\n",
    "print(f'test R2: {r2_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best test performance so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'numer of parameterrs for window length of {seq_length}: {max(ridge.coef_.shape) + len(ridge.intercept_)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So conclusion:\n",
    "- Simple Linear regression on all features 1 timpestep: good performance\n",
    "- Lasso regression on all features 1 timpestep: even slightly better performance\n",
    "- cross validation: does not yield a better performance\n",
    "- idea: for window trainig time info (with sin and cos) drop out to prevent overfitting on this! this results in the best test performance thusfar when using ridge with seq length of 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip as of now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/svm.html#svm-regression\n",
    "\n",
    "\n",
    "C and epsilon to be optimised => cross validation ideally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_nonlin = make_pipeline(StandardScaler(), SVR(kernel = 'linear', C = 100, epsilon = 0.2))\n",
    "svr_nonlin.fit(X_train, y_train)\n",
    "y_train_SVR = svr_nonlin.predict(X_train)\n",
    "r2_SVR = r2_score(y_train, y_train_SVR)\n",
    "print('trainig score SVR: ' + str(r2_SVR))\n",
    "\n",
    "y_test_SVR = svr_nonlin.predict(X_test)\n",
    "r2_SVR_test = r2_score(y_test, y_test_SVR)\n",
    "print('trainig score SVR: ' + str(r2_SVR_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "y_full.plot(ax = ax, ylabel = 'C* [mm]', label = 'PDM')\n",
    "plt.plot(X_train.index, y_train_SVR, label = 'Train', alpha = 0.7)\n",
    "plt.plot(X_test.index, y_test_SVR, label = 'Test', alpha = 0.7)\n",
    "ax.legend()\n",
    "ax.set_title('SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

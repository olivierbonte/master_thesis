{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models for the observation operator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the complex LSTM model, we'll first check the performance of much simpler ML models: linear regression and ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import dask\n",
    "import joblib\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, RidgeCV,\n",
    "                                   LassoCV, Lasso)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "pad = Path(os.getcwd())\n",
    "if pad.name == \"ml_observation_operator\":\n",
    "    pad_correct = pad.parent\n",
    "    os.chdir(pad_correct)\n",
    "from functions.PDM import PDM\n",
    "from functions.pre_processing import reshape_data, reshaped_to_train_test\n",
    "from functions.ml_utils import general_sklearn_model\n",
    "from functions.plotting_functions import plot_Cstar_model\n",
    "SEED = 1234\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "exec_hyperopt_tuning = False #SET TO TRUE TO RUN FULL NOTEBOOK\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run \"ml_observation_operator/data_load_in.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ML_data_pad = Path(\"data/Zwalm_data/ML_data\")\n",
    "X_full_all = pd.read_pickle(ML_data_pad/\"X_full_all.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(ML_data_pad/\"y_train.pkl\")\n",
    "y_test = pd.read_pickle(ML_data_pad/\"y_test.pkl\")\n",
    "y_full = pd.read_pickle(ML_data_pad/\"y_full.pkl\")\n",
    "\n",
    "Cstar = pd.read_pickle(ML_data_pad/\"Cstar.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full set of possible features: Forest, Pasture, Agriculture and a combination of pasture and agricculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_corr = X_full_all.corr()\n",
    "features_corr.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in both full and the smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(ML_data_pad/\"X_train.pkl\")\n",
    "X_test = pd.read_pickle(ML_data_pad/\"X_test.pkl\")\n",
    "X_full = pd.read_pickle(ML_data_pad/\"X_full.pkl\")\n",
    "display(X_full.head())\n",
    "print(X_full.shape)\n",
    "\n",
    "X_train_small = pd.read_pickle(ML_data_pad/\"X_train_small.pkl\")\n",
    "X_test_small = pd.read_pickle(ML_data_pad/\"X_test_small.pkl\")\n",
    "X_full_small = pd.read_pickle(ML_data_pad/\"X_full_small.pkl\")\n",
    "display(X_full_small.head())\n",
    "\n",
    "## EXPERIMENT OF 08/05: DROP DESCENDING (is conveyes the same information as ascending): now incorporated in data_load_in\n",
    "# X_train = X_train.drop('descending',axis=1)\n",
    "# X_test = X_test.drop('descending',axis=1)\n",
    "# X_full = X_full.drop('descending',axis=1)\n",
    "\n",
    "# X_train_small = X_train_small.drop('descending',axis=1)\n",
    "# X_test_small = X_test_small.drop('descending',axis=1)\n",
    "# X_full_small = X_full_small.drop('descending',axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24/03/2023: drop delta_t feature as experiment (since this will not lead to better perfomance if not multiple timesteps included). From 0.7 to around 0.74 in test score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep delta_t for the time window methods!\n",
    "X_train_dt = X_train.copy()\n",
    "X_test_dt = X_test.copy()\n",
    "X_full_dt = X_full.copy()\n",
    "\n",
    "X_train_small_dt = X_train_small.copy()\n",
    "X_test_small_dt = X_test_small.copy()\n",
    "X_full_small_dt = X_full.copy()\n",
    "\n",
    "X_train = X_train.drop('delta_t',axis = 1)\n",
    "X_test = X_test.drop('delta_t',axis = 1)\n",
    "X_full = X_full.drop('delta_t',axis = 1)\n",
    "\n",
    "X_train_small = X_train_small.drop('delta_t',axis = 1)\n",
    "X_test_small = X_test_small.drop('delta_t',axis = 1)\n",
    "X_full_small = X_full_small.drop('delta_t',axis = 1)\n",
    "display(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea of linear regression as observation operator alreayd applied in Auber(cf. [obsidian](C:\\Users\\olivi\\Documents\\ob_obsidian\\DA\\Aubert_SM_DA_in_conceptual_model.md) and https://www.sciencedirect.com/science/article/pii/S0022169403002294?via%3Dihub )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include forest in the equation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2 \n",
    "\n",
    "Calculated adjusted $R^2$ (=$\\bar{R}^2$) from regular $R^2$ as:\n",
    "$$\n",
    "{\\displaystyle {\\bar {R}}^{2}=1-(1-R^{2}){n-1 \\over n-p}}\n",
    "$$\n",
    "\n",
    "with $n$ the number of variables  and $p$ the number parameters "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 input, 1 output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very basic idea: 1 input timestep to 1 output timestep without normalisation (not covered in thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(2,1, constrained_layout = True)\n",
    "linreg, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train, X_test, y_train, y_test, X_train.index,X_test.index, Cstar\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 08/05: drop the descending one (as it actually conveyes the same information as ascending! they are 100% correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linreg_norm, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "#     LinearRegression(), X_train.drop(['descending'],axis=1), X_test.drop(['descending'],axis =1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index,X_test.index, Cstar, normalisation = True, save_predictions = True, pad = pad\n",
    "# )\n",
    "# fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add normalisation to input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad = Path('data/ml_obs_op_data/lin_reg/full_data')\n",
    "font_size = 13\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "linreg_norm, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index,X_test.index, Cstar, normalisation = True, save_predictions = True, pad = pad\n",
    ")\n",
    "ax.set_title('Linear regression')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend(['PDM','Train','Test'],loc = 'upper right')\n",
    "display(fig)\n",
    "pad_pres = Path('Figures/presentation_12_04')\n",
    "if not os.path.exists(pad_pres):\n",
    "    os.makedirs(pad_pres)\n",
    "fig.savefig(pad_pres/'lin_reg.svg',format = 'svg', transparent = True)\n",
    "plt.rcParams.update(matplotlib.rcParamsDefault)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the sin and cos feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad = Path('data/ml_obs_op_data/lin_reg/full_data_no_time')\n",
    "linreg_drop, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_train.drop(['year_sin','year_cos'], axis = 1),\n",
    "     X_test.drop(['year_sin','year_cos'], axis = 1), y_train.values.reshape(-1,1),y_test.values.reshape(-1,1),X_train.index, X_test.index, Cstar, save_predictions = True, pad = pad\n",
    ")\n",
    "fig                                             "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the forest related features (but keep time related feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_no_forest = X_train.loc[:,~X_train.columns.str.endswith('Forest')]\n",
    "X_test_no_forest = X_test.loc[:,~X_test.columns.str.endswith('Forest')]\n",
    "pad = Path('data/ml_obs_op_data/lin_reg/full_data_no_forest')\n",
    "linreg_drop_forest, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_train_no_forest, X_test_no_forest,\n",
    "    y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),\n",
    "    X_train.index, X_test.index, Cstar, save_predictions = True, pad = pad\n",
    ")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion on normalisation: not really necessary in this case, basically no difference in performance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coef_dict =  {}\n",
    "for i, param in enumerate(X_train.columns.to_list()):\n",
    "    coef_dict[param] = linreg.coef_[i]\n",
    "pd_coef = pd.DataFrame(coef_dict, index =[0])\n",
    "pd_coef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a lot of weight being given to the sinus feature!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now also try with the dataset with less features: only the lumped pasture and agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_small, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train_small,X_test_small, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index,X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "#ax.plot(X_full.index, X_full['year_sin']*150+310, label = 'Sine wave')\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse performance than on the more full set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_small, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train_small.drop(['year_sin','year_cos'],axis =1),\n",
    "    X_test_small.drop(['year_sin','year_cos'],axis =1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar,normalisation = True\n",
    ")\n",
    "ax.set_title('Normalised Linear Regression: no time features')\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly a lot of dependence on the sinus feature to get decent results!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just an experiment on 08/05/2022: principe component regression (does not work well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pcregr_pipe = make_pipeline(PCA(n_components = 0.9),LinearRegression())\n",
    "pcreg_norm, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    pcregr_pipe, X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index,X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### several input timesteps, 1 output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 5\n",
    "linreg_window, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train_dt, X_test_dt, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length\n",
    ")\n",
    "ax.set_title(\"Window LinearRegression on max # features\")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So linear regression quite clearly overfits on the window data => idea of trying ridge regression\n",
    "\n",
    "Also try on the smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_window_small, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train_small_dt, X_test_small_dt, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length\n",
    ")\n",
    "ax.set_title(\"Window Linear Regression on the lumped dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So worse performance by including more timesteps! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine several models in one picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,1, figsize = (9,9), constrained_layout = True)\n",
    "linreg_norm, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index,X_test.index, Cstar, normalisation = True, fig = fig, ax = axes[0]\n",
    ")\n",
    "axes[0].set_title('(a)')\n",
    "axes[0].set_xlabel('')\n",
    "\n",
    "linreg_drop, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_train.drop(['year_sin','year_cos'], axis = 1),\n",
    "     X_test.drop(['year_sin','year_cos'], axis = 1), y_train.values.reshape(-1,1),y_test.values.reshape(-1,1),X_train.index, X_test.index, Cstar, fig = fig, ax = axes[1]\n",
    ")\n",
    "axes[1].set_title('(b)')\n",
    "axes[1].set_xlabel('')\n",
    "\n",
    "linreg_drop_forest, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     LinearRegression(), X_train_no_forest, X_test_no_forest,\n",
    "    y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),\n",
    "    X_train.index, X_test.index, Cstar, fig = fig, ax = axes[2]\n",
    ")\n",
    "axes[2].set_title('(c)')\n",
    "axes[2].set_xlabel('')\n",
    "linreg_window, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    LinearRegression(), X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length, fig = fig, ax = axes[3]\n",
    ")\n",
    "axes[3].set_title('(d)')\n",
    "axes[3].set_xlabel('Time')\n",
    "figpad = Path('Figures/Figures_chapter_ML_obs_op')\n",
    "if not os.path.exists(figpad):\n",
    "    os.makedirs(figpad)\n",
    "fig.savefig(figpad/'Cstar_pred_LR.pdf',format = 'pdf', bbox_inches = 'tight')\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso regression\n",
    "\n",
    "L2 normalisation. Well explained in https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification \n",
    "\n",
    "Aslo L1 normalisation ncan be tried with Lasso Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html automatic best hyperparamter $\\alpha$ (often called $\\lambda$) for regularisation by appyling crosss validation. cv = 5 is 5-fold cross-validation within the training set\n",
    "\n",
    "https://reader.elsevier.com/reader/sd/pii/S0020025511006773?token=CDA1B56A3E63442F4A577A86F3FE9F199758615BFC990A1BA04E59DF8C8A4DDC652ED7FE300E09F449B8B984D40426C5&originRegion=eu-west-1&originCreation=20230510164928  Stresses to NOT shuffle when analysing time series!\n",
    "\n",
    "Note: scikitlearn solves a slightly different objective function than the one used in the dissertation description. To convert to thesis notation, multiply current alpha value of lasso times $2n_{samples}$ (form training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 input 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = False)#, shuffle=True, random_state=SEED) #10/05: siwth to False\n",
    "n_train = X_train.shape[0]\n",
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "names = [\"ridge\",\"lasso\"]\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "     ridge, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "          model, X_train, X_test, y_train,\n",
    "          y_test, X_train.index, X_test.index, Cstar\n",
    "     )\n",
    "     ax.set_title(names[i] + r' with $\\alpha = $' +  f'{model.alpha_}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quite interesting: in regulrisation, it excludes VHPasture and LAIAgriculture!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse than linear regression: both for Lasso and Ridge (but Lasso performs sligthly better)\n",
    "\n",
    "Include normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "models_norm_dict = {}\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "     models_norm_dict[names[i]], r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "          model, X_train.values, X_test.values, y_train.values.reshape(-1,1),\n",
    "          y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    "     )\n",
    "     ax.set_title('Normalised ' + names[i] + r'with $\\alpha = $' +  f'{model.alpha_}')\n",
    "     display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lasso = models_norm_dict['lasso'].alpha_*2*n_train\n",
    "print(alpha_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_weights_pd = pd.DataFrame(models_norm_dict['lasso'].coef_.reshape(1,-1),columns = X_full.columns)\n",
    "lasso_weights_pd "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regularisatio sets VH pasture and LAI Agriculture to zero! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Slightly better performance with normalisation. Lasso normalisation has highest test score thus far. Shuffle the data despite being normally against the rules here.~~\n",
    "\n",
    "Drop time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "models_no_DOY_dict = {}\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "    models_no_DOY_dict[names[i]], r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "        model , X_train.drop(['year_sin','year_cos'], axis = 1),\n",
    "        X_test.drop(['year_sin','year_cos'], axis = 1), y_train.values.reshape(-1,1),\n",
    "        y_test.values.reshape(-1,1),X_train.index, X_test.index, Cstar, normalisation = True\n",
    "    )\n",
    "    ax.set_title('Normalised ' + names[i] + r' no time features with $\\alpha = $' +  f'{model.alpha_}')\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_weights_noDOY_pd = pd.DataFrame(models_no_DOY_dict['lasso'].coef_.reshape(1,-1),columns = X_full.columns[0:-2])\n",
    "lasso_weights_noDOY_pd "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now VH agriculture is set to 0 for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "lasso = LassoCV(alphas = np.logspace(-3,3,100), cv = kf)\n",
    "models_no_forest_dict = {}\n",
    "for i, model in enumerate([ridge, lasso]):\n",
    "    models_no_forest_dict[names[i]], r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "        model, X_train_no_forest, X_test_no_forest,\n",
    "        y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),\n",
    "        X_train.index, X_test.index, Cstar, normalisation = True #normalisation = slightly better performance\n",
    "    )\n",
    "    ax.set_title('Normalised RidgeCV drop forest')\n",
    "    ax.set_title('Normalised ' + names[i] + r' drop forest with $\\alpha = $' +  f'{model.alpha_}')\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_weights_no_forest = pd.DataFrame(models_no_forest_dict['lasso'].coef_.reshape(1,-1),columns = X_train_no_forest.columns)\n",
    "lasso_weights_no_forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it sets LAI agriculture to 0!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple inputs, 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 5 #vs. 30 on the slides for Niko 05/04\n",
    "ridge_window, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     ridge, X_train, X_test, y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length\n",
    ")\n",
    "ax.set_title(r\"Window Ridge regression: $\\alpha = $\" +f\"{ridge_window.alpha_}\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 5 #Adapted to 5 on 08/04/2023 instead of 30\n",
    "ridge_window, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "     ridge, X_train_dt.drop(['year_sin','year_cos'],axis = 1), X_test_dt.drop(['year_sin','year_cos'],axis = 1), y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length\n",
    ")\n",
    "ax.set_title('Ridge window no time features')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 5 #vs. 30 on the slides for Niko 05/04\n",
    "lasso_window, r2_train, r2_test, fig, ax, lasso_tau5_train, lasso_tau5_test = general_sklearn_model(\n",
    "     lasso, X_train, X_test, y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length, return_predictions=True\n",
    ")\n",
    "ax.set_title(r\"Lasso regression: $\\alpha = $\" + f\"{lasso_window.alpha_}\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_window.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_weights_tau_5 = pd.DataFrame(lasso_window.coef_.reshape(seq_length,-1),columns =X_full.columns)\n",
    "display(lasso_weights_tau_5)\n",
    "print(np.sum(np.isclose(lasso_weights_tau_5.values,0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So interesting: sets for sin and cos nearly all values to zero!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation for best model structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for the full dataset:\n",
    "- per model (so Ridge, Lasso and normal Linear regression) find the best set of hyperparameters:\n",
    "    - sequence length of input: 1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60\n",
    "    - forest or no forest\n",
    "    - time of no time\n",
    "    - range of alpha values (include 0 = linear regression is included!)\n",
    "Normalisation is given to inputs to avoid problems with trainig of the algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "simplefilter(\"ignore\", category=UserWarning)\n",
    "nr_folds = 4\n",
    "n_train = X_train.shape[0]\n",
    "alpha_range = np.concatenate([np.logspace(-3,3,100),np.array([0])]) \n",
    "range_seq_length = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60])#2**np.arange(1,7)#np.arange(1,100,10)\n",
    "range_data_size = ['large','small']\n",
    "range_forest = [True, False]\n",
    "range_time_goniometr = [True, False]\n",
    "model_names = ['Ridge', 'Lasso']\n",
    "nr_options = len(range_seq_length)*len(range_forest)*len(range_time_goniometr)*len(alpha_range)*len(model_names)*len(range_data_size)\n",
    "col_names = ['model','seq_length','forest_bool','time_bool','alpha','r2_val_mean','r2_val_sd']\n",
    "pd_hyperparam = pd.DataFrame(columns=col_names, index = range(0,nr_options))\n",
    "iter = 0\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "X_full_norm = pd.DataFrame(X_scaler.transform(X_full), columns = X_full.columns)\n",
    "X_scaler_small = StandardScaler()\n",
    "X_scaler_small.fit(X_train_small)\n",
    "X_full_small_norm = pd.DataFrame(X_scaler_small.transform(X_full_small), columns = X_full_small.columns)\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(y_train.values.reshape(-1,1))\n",
    "y_full_norm = y_scaler.transform(y_full.values.reshape(-1,1))\n",
    "if exec_hyperopt_tuning:\n",
    "    for seq_length in range_seq_length:\n",
    "        for data_size in range_data_size:\n",
    "            for time_goniometr in range_time_goniometr:\n",
    "                for forest in range_forest:\n",
    "                    if data_size == 'large':\n",
    "                        X_temp = X_full_norm.copy()\n",
    "                    else:\n",
    "                        X_temp = X_full_small_norm.copy()\n",
    "                    if not time_goniometr:\n",
    "                        X_temp = X_temp.drop(['year_sin','year_cos'],axis = 1)\n",
    "                    if data_size == 'large':\n",
    "                        if not forest:\n",
    "                            X_temp = X_temp.loc[:,~X_temp.columns.str.endswith('Forest')]\n",
    "                    X_window, y_window, t_window = reshape_data(\n",
    "                        X_temp.values,y_full_norm,\n",
    "                        X_full.index.values, seq_length\n",
    "                    )\n",
    "                    (X_window_train, X_window_test, y_window_train, y_window_test, \n",
    "                    t_window_train, t_window_test) = reshaped_to_train_test(\n",
    "                        X_window, y_window, t_window, seq_length, n_train, output_dim = 2\n",
    "                    )\n",
    "                    for alpha in alpha_range:\n",
    "                        for model_name in model_names:\n",
    "                            kf = KFold(nr_folds, shuffle = False)#no shuffle!!!\n",
    "                            r2_val_list = []\n",
    "                            for i, (train_index, test_index) in enumerate(kf.split(X_window_train)):\n",
    "                                if model_name == 'Lasso':\n",
    "                                    model = Lasso(alpha = alpha)\n",
    "                                else:\n",
    "                                    model = Ridge(alpha = alpha)\n",
    "                                #model_temp, r2_train, r2_val, fig, ax = general_sklearn_model(\n",
    "                                delayed_result = dask.delayed(general_sklearn_model)(\n",
    "                                    model, X_window_train[train_index], X_window_train[test_index],\n",
    "                                    y_window_train[train_index],y_window_train[test_index], \n",
    "                                    t_window_train[train_index], t_window_train[test_index], Cstar,\n",
    "                                    print_output = False\n",
    "                                )\n",
    "                                r2_val_list.append(delayed_result)\n",
    "                                #r2_val_list.append(r2_val)\n",
    "                            r2_vals = dask.compute(*r2_val_list)\n",
    "                            r2_vals = [r2_vals[i][2] for i in range(len(r2_vals))]\n",
    "                            pd_hyperparam.iloc[iter,:] = [model_name,seq_length, forest, time_goniometr,\n",
    "                                                        alpha, np.mean(r2_vals),np.std(r2_vals)]\n",
    "                            iter = iter + 1 \n",
    "                            if iter%100 == 0:\n",
    "                                print(f'Iteration {iter} out of {nr_options} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exec_hyperopt_tuning:\n",
    "    pd_hyperparam.to_csv(\"data/ml_obs_op_data/lin_reg_hyperparam_cv.csv\", index = False)\n",
    "else:\n",
    "    pd_hyperparam = pd.read_csv(\"data/ml_obs_op_data/lin_reg_hyperparam_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd_hyperparam))\n",
    "pd_hyperparam.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_best = pd_hyperparam[pd_hyperparam['r2_val_mean'].max() == pd_hyperparam['r2_val_mean']]\n",
    "hyperparam_best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain a model with the above information on the entire training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hyperparam_best['model'].values[0]\n",
    "alpha = hyperparam_best['alpha'].values[0]\n",
    "seq_length = hyperparam_best['seq_length'].values[0]\n",
    "time_goniometr = hyperparam_best['time_bool'].values[0]\n",
    "forest = hyperparam_best['forest_bool'].values[0]\n",
    "X_temp_train = X_train.copy()\n",
    "X_temp_test = X_test.copy()\n",
    "if not time_goniometr:\n",
    "    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not forest:\n",
    "    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')]\n",
    "    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')]\n",
    "if model == 'Ridge':\n",
    "    model = Ridge(alpha = alpha)\n",
    "elif model == 'Lasso':\n",
    "    model = Lasso(alpha = alpha)\n",
    "out_dict = general_sklearn_model(\n",
    "    model, X_temp_train, X_temp_test, y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length\n",
    ")\n",
    "display(out_dict[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ridge regresion even after optimisation of hyperparameters, does not perform better than a simple linear regression on al features with a window base approach. Note that in fact a simple linear regression without the window base approach outperforms this approach on test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam.sort_values('r2_val_mean',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_hyperparam_lasso = pd_hyperparam[pd_hyperparam['model'] == 'Lasso']\n",
    "pd_hyperparam_lasso.sort_values('r2_val_mean',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_lasso_best = pd_hyperparam_lasso[\n",
    "    pd_hyperparam_lasso['r2_val_mean'].max() == pd_hyperparam_lasso['r2_val_mean']\n",
    "]\n",
    "model = hyperparam_lasso_best['model'].values[0]\n",
    "alpha = hyperparam_lasso_best['alpha'].values[0]\n",
    "seq_length = hyperparam_lasso_best['seq_length'].values[0]\n",
    "time_goniometr = hyperparam_lasso_best['time_bool'].values[0]\n",
    "forest = hyperparam_lasso_best['forest_bool'].values[0]\n",
    "X_temp_train = X_train.copy()\n",
    "X_temp_test = X_test.copy()\n",
    "if not time_goniometr:\n",
    "    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not forest:\n",
    "    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')]\n",
    "    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')]\n",
    "if model == 'Ridge':\n",
    "    model = Ridge(alpha = alpha)\n",
    "elif model == 'Lasso':\n",
    "    model = Lasso(alpha = alpha)\n",
    "out_dict = general_sklearn_model(\n",
    "    model, X_temp_train, X_temp_test, y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length\n",
    ")\n",
    "display(out_dict[3])\n",
    "display(out_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_sorted = pd_hyperparam[pd_hyperparam['time_bool'] == False].sort_values('r2_val_mean',ascending = False)\n",
    "no_time_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_best = pd.DataFrame(no_time_sorted.iloc[0,:].values.reshape(1,-1), columns = no_time_sorted.columns)\n",
    "hyperparam_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_best = pd.DataFrame(no_time_sorted.iloc[0,:].values.reshape(1,-1), columns = no_time_sorted.columns)\n",
    "model = hyperparam_best['model'].values[0]\n",
    "alpha = hyperparam_best['alpha'].values[0]\n",
    "seq_length = hyperparam_best['seq_length'].values[0]\n",
    "time_goniometr = hyperparam_best['time_bool'].values[0]\n",
    "forest = hyperparam_best['forest_bool'].values[0]\n",
    "X_temp_train = X_train.copy()\n",
    "X_temp_test = X_test.copy()\n",
    "if not time_goniometr:\n",
    "    X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "    X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not forest:\n",
    "    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')]\n",
    "    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')]\n",
    "if model == 'Ridge':\n",
    "    model = Ridge(alpha = alpha)\n",
    "elif model == 'Lasso':\n",
    "    model = Lasso(alpha = alpha)\n",
    "else:\n",
    "    raise ValueError('model should be Lasso or Ridge')\n",
    "\n",
    "#save output\n",
    "pad = Path('data/ml_obs_op_data/ridge/window')\n",
    "ridge_w_nt, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    model, X_temp_train, X_temp_test, y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = seq_length, save_predictions=True, pad = pad\n",
    ")\n",
    "ax.set_title('Ridge window regression')\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best test performance so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_w_nt.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'numer of parameterrs for window length of {seq_length}: {max(ridge_w_nt.coef_.shape) + len(ridge_w_nt.intercept_)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment of no time for Lasso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_sorted_lasso = pd_hyperparam_lasso[pd_hyperparam_lasso['time_bool'] == False].sort_values('r2_val_mean',ascending = False)\n",
    "display(no_time_sorted_lasso.head(5))\n",
    "X_temp_train = X_train.copy()\n",
    "X_temp_test = X_test.copy()\n",
    "X_temp_train = X_temp_train.drop(['year_sin','year_cos'],axis = 1)\n",
    "X_temp_test = X_temp_test.drop(['year_sin','year_cos'],axis = 1)\n",
    "if not no_time_sorted_lasso['forest_bool'].iloc[0]:\n",
    "    X_temp_train = X_temp_train.loc[:,~X_temp_train.columns.str.endswith('Forest')]\n",
    "    X_temp_test = X_temp_test.loc[:,~X_temp_test.columns.str.endswith('Forest')]\n",
    "model = Lasso(alpha = no_time_sorted_lasso['alpha'].iloc[0])\n",
    "pad = Path('data/ml_obs_op_data/lasso/window')\n",
    "lasso_w_nt, r2_train, r2_test, fig, ax, lasso_nt_train, lasso_nt_test = general_sklearn_model(\n",
    "    model, X_temp_train, X_temp_test, y_train.values.reshape(-1,1),y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True, seq_length = no_time_sorted_lasso['seq_length'].iloc[0], save_predictions=True, pad = pad, return_predictions=True\n",
    ")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_weights_cv = pd.DataFrame(lasso_w_nt.coef_.reshape(no_time_sorted_lasso['seq_length'].iloc[0],-1),columns =X_full.columns[0:-2])\n",
    "lasso_weights_cv.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(lasso_weights_cv.values == 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So conclusion:\n",
    "- Simple Linear regression on all features 1 timpestep: good performance\n",
    "- Lasso regression on all features 1 timpestep: even slightly better performance\n",
    "- cross validation: does not yield a better performance\n",
    "- idea: for window trainig time info (with sin and cos) drop out to prevent overfitting on this! this results in the best test performance thusfar when using ridge with seq length of 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the two models that will be used in the disseration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_tau5_train.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, figsize = (9,5), constrained_layout = True)\n",
    "plot_Cstar_model(lasso_tau5_train.values, lasso_tau5_test.values, lasso_tau5_train.index, lasso_tau5_test.index, Cstar, X_full.index, fig, axes[0])\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_title('(a)')\n",
    "plot_Cstar_model(lasso_nt_train.values, lasso_nt_test.values, lasso_nt_train.index, lasso_nt_test.index, Cstar, X_full.index, fig, axes[1])\n",
    "axes[1].set_title('(b)')\n",
    "axes[1].set_xlabel('Time')\n",
    "display(fig)\n",
    "fig.savefig(figpad/'Cstar_pred_lasso.pdf',format = 'pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/svm.html#svm-regression  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place to save\n",
    "pad = Path('data/ml_obs_op_data/SVR')\n",
    "if not os.path.exists(pad):\n",
    "    os.makedirs(pad)\n",
    "\n",
    "# Full dataset\n",
    "svr = SVR(kernel = 'linear', C = 1, epsilon = 0.1) #default values!\n",
    "svr_lin, r2_train, r2_test, fig, ax = general_sklearn_model(svr, X_train, X_test, y_train.values.reshape(-1, 1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True)\n",
    "ax.set_title('Linear SVR on full trainig set')\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now exclude time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_lin_nt, r2_train, r2_test, fig, ax = general_sklearn_model(\n",
    "    svr, X_train.drop(['year_sin','year_cos'],axis =1), X_test.drop(['year_sin','year_cos'],axis = 1), y_train.values.reshape(-1, 1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "ax.set_title('Linear SVR on full trainig set without time')\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performes worse than normal linear regression\n",
    "\n",
    "\n",
    "C and epsilon to be optimised => cross validation ideally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel = 'linear')\n",
    "svr_gs = GridSearchCV(svr, param_grid = {\n",
    "    'C':np.logspace(-10,3,14),\n",
    "    'epsilon':np.logspace(-3,1,20),\n",
    "}, scoring = 'r2', cv = 5, n_jobs = -1\n",
    ") #5 fold CV without shuffling\n",
    "if exec_hyperopt_tuning: \n",
    "    svr_gs_tuple_out = general_sklearn_model(\n",
    "        svr_gs, X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),X_train.index, X_test.index,Cstar,normalisation = True\n",
    "    )\n",
    "    joblib.dump(svr_gs_tuple_out,pad/'svr_optim_linear.joblib')\n",
    "else:\n",
    "    svr_gs_tuple_out = joblib.load(pad/'svr_optim_linear.joblib')\n",
    "svr_gs_out = svr_gs_tuple_out[0]\n",
    "r2_train = svr_gs_tuple_out[1]\n",
    "r2_test = svr_gs_tuple_out[2]\n",
    "fig = svr_gs_tuple_out[3]\n",
    "ax = svr_gs_tuple_out[4]\n",
    "epsilon  = svr_gs_out.best_estimator_.epsilon\n",
    "c = svr_gs_out.best_estimator_.C\n",
    "ax.set_title(r'Optimised linear SVR: $\\epsilon$ = '+ f'{epsilon}' + r', $C$ = ' + f'{c}')\n",
    "if not exec_hyperopt_tuning:\n",
    "    print(r2_train)\n",
    "    print(r2_test)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_gs_out.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear: RBF kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really analogous to the work of: https://ieeexplore.ieee.org/document/9451176 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel = 'rbf', C = 1, epsilon = 0.1, gamma ='auto') #default test (auto means gamma = 1/n_features)\n",
    "svr_rbf_out, r2_train, r2_test, fig, ax = general_sklearn_model(svr_rbf, X_train, X_test, y_train.values.reshape(-1, 1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True)\n",
    "ax.set_title('SVR RBF full dataset')\n",
    "print(f'gamma: {1/n_train}')\n",
    "display(svr_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_out, r2_train, r2_test, fig, ax = general_sklearn_model(svr_rbf, X_train.drop(['year_sin','year_cos'],axis = 1), X_test.drop(['year_sin','year_cos'],axis = 1), y_train.values.reshape(-1, 1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True)\n",
    "ax.set_title('SVR RBF full dataset no time features')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again perform a GridSearch to determine optimal values of $\\gamma,\\epsilon$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel = 'rbf')\n",
    "svr_gs_rbf = GridSearchCV(svr_rbf, param_grid = {\n",
    "    'C':np.logspace(-10,3,14),\n",
    "    'epsilon':np.logspace(-3,1,20),\n",
    "    'gamma':np.logspace(-5,5,50)\n",
    "}, scoring = 'r2', cv = 5, n_jobs = -1, verbose = 3\n",
    ") #5 fold CV without shuffling\n",
    "if exec_hyperopt_tuning: \n",
    "    svr_gs_tuple_out = general_sklearn_model(\n",
    "        svr_gs_rbf, X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),X_train.index, X_test.index,Cstar,normalisation = True\n",
    "    )\n",
    "    joblib.dump(svr_gs_tuple_out,pad/'svr_optim_rbf.joblib')\n",
    "else:\n",
    "    svr_gs_tuple_out = joblib.load(pad/'svr_optim_rbf.joblib')\n",
    "svr_gs_rbf_out = svr_gs_tuple_out[0]\n",
    "r2_train = svr_gs_tuple_out[1]\n",
    "r2_test = svr_gs_tuple_out[2]\n",
    "fig = svr_gs_tuple_out[3]\n",
    "ax = svr_gs_tuple_out[4]\n",
    "epsilon  = svr_gs_rbf_out.best_estimator_.epsilon\n",
    "c = svr_gs_rbf_out.best_estimator_.C\n",
    "gamma = svr_gs_rbf_out.best_estimator_.gamma\n",
    "ax.set_title(r'Optimised RBF SVR: $\\epsilon$ = '+ f'{epsilon}' + r', $C$ = ' + f'{c}' + r', $\\gamma$ = ' + f'{gamma}')\n",
    "if not exec_hyperopt_tuning:\n",
    "    print(r2_train)\n",
    "    print(r2_test)\n",
    "    display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply idea of Rains et al here: only take in VV (+ orbit info) on small dataset combined with LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_rains = SVR(kernel = 'rbf', epsilon = 0.1)\n",
    "svr_gs_rbf_rains = GridSearchCV(svr_rbf_rains, param_grid = {\n",
    "    'C':[0.001, 0.01, 0.1, 1, 10, 20, 30, 40, 50, 100],\n",
    "    'gamma':np.logspace(-3,1,5),\n",
    "}, scoring = 'r2', cv = 5, n_jobs = -1, verbose = 3\n",
    ")\n",
    "\n",
    "if exec_hyperopt_tuning: \n",
    "    svr_gs_tuple_out = general_sklearn_model(\n",
    "        svr_gs_rbf_rains, X_train_small.drop(['year_sin','year_cos','VH_past_agr'], axis = 1), X_test_small.drop(['year_sin','year_cos','VH_past_agr'], axis = 1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),X_train.index, X_test.index,Cstar,normalisation = True\n",
    "    )\n",
    "    # svr_gs_tuple_out = general_sklearn_model(\n",
    "    #     svr_gs_rbf_rains, X_train_small.drop(['VH_past_agr'], axis = 1), X_test_small.drop(['VH_past_agr'], axis = 1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),X_train.index, X_test.index,Cstar,normalisation = True\n",
    "    # )\n",
    "    joblib.dump(svr_gs_tuple_out,pad/'svr_optim_rbf_rains.joblib')\n",
    "else:\n",
    "    svr_gs_tuple_out = joblib.load(pad/'svr_optim_rbf_rains.joblib')\n",
    "svr_gs_rbf_out = svr_gs_tuple_out[0]\n",
    "r2_train = svr_gs_tuple_out[1]\n",
    "r2_test = svr_gs_tuple_out[2]\n",
    "fig = svr_gs_tuple_out[3]\n",
    "ax = svr_gs_tuple_out[4]\n",
    "epsilon  = svr_gs_rbf_out.best_estimator_.epsilon\n",
    "c = svr_gs_rbf_out.best_estimator_.C\n",
    "gamma = svr_gs_rbf_out.best_estimator_.gamma\n",
    "ax.set_title(r'Optimised RBF SVR: $\\epsilon$ = '+ f'{epsilon}' + r', $C$ = ' + f'{c}' + r', $\\gamma$ = ' + f'{gamma}')\n",
    "if not exec_hyperopt_tuning:\n",
    "    print(r2_train)\n",
    "    print(r2_test)\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch it around to forward observation operator idea: on small data set use C* + LAI + DOY -> gamma_0_VV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_gamma0_VV = X_full_small['VV_past_agr']\n",
    "# Cstar_forward = Cstar.reset_index()\n",
    "# Cstar_forward = Cstar_forward.rename(columns = {'Time':'t'})\n",
    "# Cstar_forward = Cstar_forward.set_index('t')\n",
    "# features_forward = pd.merge(X_full_small, Cstar_forward, how = 'left', on = 't')\n",
    "# features_forward = features_forward.drop(['VV_past_agr','VH_past_agr'], axis = 1)\n",
    "# features_forward_small = features_forward.drop(['year_sin','year_cos','ascending'], axis = 1)\n",
    "# svr_rbf_test = SVR(kernel = 'rbf', epsilon = 0.1)\n",
    "# svr_gs_rbf_test = GridSearchCV(svr_rbf_test, param_grid = {\n",
    "#     'C':np.logspace(-10,3,14),\n",
    "#     'gamma':np.logspace(-5,5,50)\n",
    "# }, scoring = 'r2', cv = 5, n_jobs = -1, verbose = 3\n",
    "# )\n",
    "# svr_test_out = svr_gs_rbf_test.fit(features_forward_small, target_gamma0_VV.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svr_test_out.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New idea of 08/05: keep epsilon at 0.1 in hyperparameter tuning (as done by Rains!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_eps = SVR(kernel = 'rbf', epsilon = 0.1)\n",
    "svr_gs_rbf_eps = GridSearchCV(svr_rbf_eps, param_grid = {\n",
    "    'C':np.logspace(-10,3,14),\n",
    "    'gamma':np.logspace(-5,5,50)\n",
    "}, scoring = 'r2', cv = 5, n_jobs = -1, verbose = 3\n",
    ")\n",
    "if exec_hyperopt_tuning: \n",
    "    svr_gs_tuple_out = general_sklearn_model(\n",
    "        svr_gs_rbf_eps, X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),X_train.index, X_test.index,Cstar,normalisation = True\n",
    "    )\n",
    "    joblib.dump(svr_gs_tuple_out,pad/'svr_optim_rbf_eps.joblib')\n",
    "else:\n",
    "    svr_gs_tuple_out = joblib.load(pad/'svr_optim_rbf_eps.joblib')\n",
    "svr_gs_rbf_eps_out = svr_gs_tuple_out[0]\n",
    "r2_train = svr_gs_tuple_out[1]\n",
    "r2_test = svr_gs_tuple_out[2]\n",
    "fig = svr_gs_tuple_out[3]\n",
    "ax = svr_gs_tuple_out[4]\n",
    "epsilon  = svr_gs_rbf_eps_out.best_estimator_.epsilon\n",
    "c = svr_gs_rbf_eps_out.best_estimator_.C\n",
    "gamma = svr_gs_rbf_eps_out.best_estimator_.gamma\n",
    "ax.set_title(r'Optimised RBF SVR: $\\epsilon$ = '+ f'{epsilon}' + r', $C$ = ' + f'{c}' + r', $\\gamma$ = ' + f'{gamma}')\n",
    "if not exec_hyperopt_tuning:\n",
    "    print(r2_train)\n",
    "    print(r2_test)\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better performance than the previous experiment where $\\epsilon$ could vary = this one is used!\n",
    "\n",
    "Now this approach is also once tried for no time features for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_eps = SVR(kernel = 'rbf', epsilon = 0.1)\n",
    "svr_gs_rbf_eps = GridSearchCV(svr_rbf_eps, param_grid = {\n",
    "    'C':np.logspace(-10,3,14),\n",
    "    'gamma':np.logspace(-5,5,50)\n",
    "}, scoring = 'r2', cv = 5, n_jobs = -1, verbose = 3\n",
    ")\n",
    "exec_hyperopt_tuning = True\n",
    "if exec_hyperopt_tuning: \n",
    "    svr_gs_tuple_out = general_sklearn_model(\n",
    "        svr_gs_rbf_eps, X_train.drop(['year_sin','year_cos'],axis = 1), X_test.drop(['year_sin','year_cos'],axis =1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1),X_train.index, X_test.index,Cstar,normalisation = True\n",
    "    )\n",
    "    joblib.dump(svr_gs_tuple_out,pad/'svr_optim_rbf_eps_no_time.joblib')\n",
    "else:\n",
    "    svr_gs_tuple_out = joblib.load(pad/'svr_optim_rbf_eps_no_time.joblib')\n",
    "svr_gs_rbf_eps_out = svr_gs_tuple_out[0]\n",
    "r2_train = svr_gs_tuple_out[1]\n",
    "r2_test = svr_gs_tuple_out[2]\n",
    "fig = svr_gs_tuple_out[3]\n",
    "ax = svr_gs_tuple_out[4]\n",
    "epsilon  = svr_gs_rbf_eps_out.best_estimator_.epsilon\n",
    "c = svr_gs_rbf_eps_out.best_estimator_.C\n",
    "gamma = svr_gs_rbf_eps_out.best_estimator_.gamma\n",
    "ax.set_title(r'Optimised RBF SVR, no DOY: $\\epsilon$ = '+ f'{epsilon}' + r', $C$ = ' + f'{c}' + r', $\\gamma$ = ' + f'{gamma}')\n",
    "if not exec_hyperopt_tuning:\n",
    "    print(r2_train)\n",
    "    print(r2_test)\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes\n",
    "\n",
    "Bayesian, non-linear model, also using the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad to save to later\n",
    "pad = Path('data/ml_obs_op_data/GPR')\n",
    "font_size = 13\n",
    "presentation_plot = False\n",
    "if presentation_plot:\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize = (9,4))\n",
    "kernel = RBF(length_scale_bounds=(1e-2,1e2)) + WhiteKernel(noise_level_bounds=(1e-1,1e3))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=100, normalize_y=True, random_state = SEED)\n",
    "gpr_pipe = make_pipeline(StandardScaler(),gpr)\n",
    "gpr_pipe_out,r2_train,r2_test,fig,ax = general_sklearn_model(\n",
    "    gpr_pipe, X_train, X_test, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, #normalisation = True, \n",
    "    save_predictions =True, pad = pad, fig = fig, ax = ax\n",
    ")\n",
    "#Also ask the standard deviations! \n",
    "y_mean, y_std = gpr_pipe_out.predict(X_full, return_std = True)\n",
    "ax.fill_between(X_full.index, y_mean - 2*y_std, y_mean + 2*y_std, color = 'lightgrey')\n",
    "# ax.set_title('GPR with RBF kernel: full dataset')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend(['PDM','Train','Test','95% CI'],loc = 'lower left')\n",
    "if presentation_plot:\n",
    "    ax.set_title('Gaussian processes')\n",
    "    fig.savefig(pad_pres/'gpr.svg',format = 'svg', transparent = True)\n",
    "    display(fig)\n",
    "    plt.rcParams.update(matplotlib.rcParamsDefault)\n",
    "else:\n",
    "    ax.set_title('')\n",
    "    fig.savefig(figpad/'gpr_predictions.pdf', format = 'pdf', bbox_inches = 'tight')\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_pipe_out[1].kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the time features\n",
    "gpr_pipe_out,r2_train,r2_test,fig,ax = general_sklearn_model(\n",
    "    gpr_pipe, X_train.drop(['year_sin','year_cos'],axis = 1), X_test.drop(['year_sin','year_cos'],axis =1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "ax.set_title('GPR with RBF kernel: full dataset no time features')\n",
    "display(fig)\n",
    "display(gpr_pipe_out[1].kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the forest features\n",
    "gpr_pipe_out,r2_train,r2_test,fig,ax = general_sklearn_model(\n",
    "    gpr_pipe, X_train_no_forest, X_test_no_forest, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "ax.set_title('GPR with RBF kernel: full dataset no forest features')\n",
    "display(fig)\n",
    "display(gpr_pipe_out[1].kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small dataset\n",
    "gpr_pipe_out,r2_train,r2_test,fig,ax = general_sklearn_model(\n",
    "    gpr_pipe, X_train_small, X_test_small, y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "ax.set_title('GPR with RBF kernel: small dataset')\n",
    "display(fig)\n",
    "display(gpr_pipe_out[1].kernel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_pipe_out,r2_train,r2_test,fig,ax = general_sklearn_model(\n",
    "    gpr_pipe, X_train_small.drop(['year_sin','year_cos'],axis = 1), X_test_small.drop(['year_sin','year_cos'],axis = 1), y_train.values.reshape(-1,1), y_test.values.reshape(-1,1), X_train.index, X_test.index, Cstar, normalisation = True\n",
    ")\n",
    "ax.set_title('GPR with RBF kernel: small dataset no DOY')\n",
    "display(fig)\n",
    "display(gpr_pipe_out[1].kernel_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that performance on small dataset without time features is almost just as bad as the linear methods!\n",
    "\n",
    "In lost notebook also based on CV hyperparameters were determined, will not be repeated here (did not yield superior results). The method based on CV is also mentioned in book on GP for ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An experiment: observation operator: from "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Basics\n",
    "\n",
    "https://www.tensorflow.org/guide/basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(x, axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "  print(\"TensorFlow **IS** using the GPU\")\n",
    "else:\n",
    "  print(\"TensorFlow **IS NOT** using the GPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Tensor` zijn onveanderlijk <-> `tf.Variable` zijn veranderlijk = nodig voor modelgewichten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable([0.0, 0.0, 0.0])\n",
    "var.assign([1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Automatic differentiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use calculus to compute the gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0) #variable since mutable\n",
    "\n",
    "def f(x):\n",
    "  y = x**2 + 2*x - 5\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape: #only works on variables\n",
    "  y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x)  # g(x) = dy/dx\n",
    "\n",
    "g_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also works for non scalars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graphs and tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate the pure tensorflow code!\n",
    "class test(tf.Module):\n",
    "  @tf.function #compiles to tf graph\n",
    "  def my_func(x):\n",
    "    print('Tracing.\\n')\n",
    "    return tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1, 2, 3])\n",
    "test_obj = test()\n",
    "# test_obj.my_func(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export for system not having python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(test_obj, 'data/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Modules, layers and models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Module` is a class for managing your `tf.Variable` objects, and the `tf.function` objects that operate on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(tf.Module):\n",
    "  def __init__(self, value):\n",
    "    self.weight = tf.Variable(value)\n",
    "\n",
    "  @tf.function\n",
    "  def multiply(self, x):\n",
    "    return x * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = MyModule(3)\n",
    "mod.weight.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.multiply(tf.constant([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './data'\n",
    "tf.saved_model.save(mod, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load(save_path)\n",
    "reloaded.multiply(tf.constant([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-2, 2, 201)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.cast(x, tf.float32) #from float64 to float32\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  y = x**2 + 2*x - 5\n",
    "  return y\n",
    "\n",
    "y = f(x) + tf.random.normal(shape=[201])\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy(), '.', label='Data')\n",
    "#so .numpy() is method to convert to numpy!\n",
    "plt.plot(x, f(x), label='Ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "\n",
    "  def __init__(self): #initialise the model! \n",
    "    # Randomly generate weight and bias terms\n",
    "    rand_init = tf.random.uniform(shape=[3], minval=0., maxval=5., seed=22)\n",
    "    # Initialize model parameters\n",
    "    #Needs to be variables since needs to have possibility to update \n",
    "    self.w_q = tf.Variable(rand_init[0])\n",
    "    self.w_l = tf.Variable(rand_init[1])\n",
    "    self.b = tf.Variable(rand_init[2])\n",
    "    print('I am initalised')\n",
    "\n",
    "  @tf.function #always need this decorator!\n",
    "  def __call__(self, x): #this allows calss to act like function\n",
    "    # Quadratic Model : quadratic_weight * x^2 + linear_weight * x + bias\n",
    "    print('I am being called as a function')\n",
    "    return self.w_q * (x**2) + self.w_l * x + self.b\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mod = model_instance(x)\n",
    "# y is a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, y, '.', label='Data')\n",
    "plt.plot(x, f(x), label='Ground truth')\n",
    "plt.plot(x, model_instance(x), label='Predictions')\n",
    "plt.title('Before training')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y):\n",
    "  return tf.reduce_mean(tf.square(y_pred - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #standard batch size!\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "list(dataset.as_numpy_iterator()) #so all pairs of features and outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\" Here equal to is uzsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset.shuffle(buffer_size = x.shape[0]).as_numpy_iterator())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly this just shuffeled the above dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size = x.shape[0]).batch(batch_size)\n",
    "batch_list = list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "\n",
    "# Format training loop\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_loss = mse_loss(model_instance(x_batch), y_batch) #so cacluate loss\n",
    "        grads = tape.gradient(batch_loss, model_instance.variables)\n",
    "        for g,v in zip(grads, model_instance.variables):\n",
    "            v.assign_sub(learning_rate*g)\n",
    "    loss = mse_loss(model_instance(x),y) #loss per epoch!\n",
    "    losses.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Mean squared error for step {epoch}: {loss.numpy():0.3f}')\n",
    "\n",
    "# Plot model results\n",
    "print(\"\\n\")\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title('MSE loss vs training iterations');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, y, '.', label='Data')\n",
    "plt.plot(x, f(x), label='Ground truth')\n",
    "plt.plot(x, model_instance(x), label='Predictions')\n",
    "plt.title('After training')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these things are availbale with Keras API!\n",
    "\n",
    "- start with `tf.keras.Sequential` for sequential group of layers! https://www.tensorflow.org/api_docs/python/tf/keras/Sequential \n",
    "- `tf.keras.layers.Dense` is the standard linear regression layer: $Y = WX + \\mathbf{b}$. Note that you CAN use an activatio function here by specifying e..g 'acitvation = relu' https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense \n",
    "\n",
    "Here we want both $x$ and $x^2$ as input!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fun = lambda x: tf.stack([x, x**2], axis=1)\n",
    "test_fun(tf.constant([3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.stack([x, x**2], axis=1)), #Wraps arbitrary expressions as a Layer object.! \n",
    "    tf.keras.layers.Dense(units=1, kernel_initializer=tf.random.normal)]) #units1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01), #SGD = Stochastic Gradient Descent\n",
    "    loss = tf.keras.losses.MSE\n",
    ")\n",
    "\n",
    "history = new_model.fit(x, y,\n",
    "                         epochs = 100,\n",
    "                         batch_size = 32,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.ylabel('Loss [Mean Squared Error]')\n",
    "plt.title('Keras training progress');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Tensors\n",
    "\n",
    "https://www.tensorflow.org/guide/tensor\n",
    "\n",
    "tensors may have 0 (scalr) or 1 (vector),2, 3... (matrix) axes\n",
    "\n",
    "`np.array` for tensor to numpy array\n",
    "\n",
    "`.shape` voor vomr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Variables\n",
    "\n",
    "https://www.tensorflow.org/guide/variable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Automatic differentiation\n",
    "\n",
    "https://www.tensorflow.org/guide/autodiff\n",
    "\n",
    "Needed for backpropagation:\n",
    "\n",
    "Network: ${\\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\\cdots f^{1}(W^{1}x)\\cdots ))}$\n",
    "\n",
    "Cost: ${\\displaystyle C(y_{i},g(x_{i}))}$\n",
    "\n",
    "\"Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer – specifically, the gradient of the weighted input of each layer, denoted by  ${\\displaystyle \\delta ^{l}}$ – from back to front.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)  #(taget, source)\n",
    "dy_dx.numpy() #dy/dx == 2*x = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both for bias and the $w$ values a gradient!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 The Sequential model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/keras/sequential_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goed voor 'plain stack', dus één input en één output tensor per laag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "    layers.Dense(2, activation = \"relu\", name = 'layer1'), #first number = dimensionality of the output space!\n",
    "    layers.Dense(3, activation = \"relu\", name = 'layer2'),\n",
    "    layers.Dense(4, name = 'layer3'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = tf.ones((3,3))\n",
    "print(x)\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can equivalent also stack the layers (one in the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alt = keras.Sequential()\n",
    "model_alt.add(layers.Dense(2, activation = 'relu'))\n",
    "model_alt.add(layers.Dense(3, activation = 'relu'))\n",
    "model_alt.add(layers.Dense(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.pop()` to remove a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alt.weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you give an input, no weights are given as these depend on the input size of your vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_alt(x)\n",
    "#now the model has been build\n",
    "model_alt.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the shape of your input in advance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(4,)))\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equivalent\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation='relu', input_shape = (4,)))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 RNN in Keras\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim = 1000, output_dim = 64))\n",
    "#1000 characters input vocab to output embedding\n",
    "\n",
    "model.add(layers.LSTM(128)) #128 internal units\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = keras.Sequential()\n",
    "model_test.add(layers.LSTM(5, input_shape = (100,11,)))\n",
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default: only return last value of the OUTPUT so that output is of size `(batch_size, units)`\n",
    "If you want value for every timestep set `return_sequences = True` and then you get output of size `(batch_size, timesteps, units)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
    "model.add(layers.GRU(256, return_sequences=True))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you also want to retrieve the (FINAl) internal states, you should specify this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab = 1000\n",
    "decoder_vocab = 2000\n",
    "\n",
    "encoder_input = layers.Input(shape=(None,))\n",
    "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
    "    encoder_input\n",
    ")\n",
    "\n",
    "# Return states in addition to output\n",
    "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
    "    encoder_embedded\n",
    ")\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder_input = layers.Input(shape=(None,))\n",
    "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
    "    decoder_input\n",
    ")\n",
    "\n",
    "# Pass the 2 states to a new LSTM layer, as initial state\n",
    "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
    "    decoder_embedded, initial_state=encoder_state\n",
    ")\n",
    "output = layers.Dense(10)(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN cell = 1 timpestap at a time processes <=> RNN default all at the same time!\n",
    "\n",
    "`RNN(LSTMCel(10))` is a RNN 'for loop' around the `LSTMCel` structure!! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe useful: https://www.tensorflow.org/guide/keras/rnn#cross-batch_statefulness however not sure that this is the goal..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic regression with Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/keras/regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frederik Kratzert LSTM for rainfall runoff modellin\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
